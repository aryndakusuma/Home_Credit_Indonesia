{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc70e10-d667-4f8f-b0b5-1a48da080efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc # Garbage Collector to help manage memory\n",
    "\n",
    "from sklearn.impute import SimpleImputer # Untuk imputasi missing value\n",
    "from sklearn.ensemble import IsolationForest # Untuk deteksi outlier\n",
    "from scipy.stats import zscore # Untuk deteksi outlier berbasis Z-score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer # Ini baris yang perlu diubah/ditambahkan\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import loguniform # Untuk distribusi C\n",
    "import warnings\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3ab7a-b0d7-4dee-9555-a5c0c331d7d4",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902ba96-d6ae-4828-8e72-972501da08f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def perform_eda(df, df_name, id_cols=[], target_col=None):\n",
    "    \"\"\"\n",
    "    Performs a standard EDA process on a given dataframe.\n",
    "    Includes: Info, Head, Missing Values, Descriptive Stats, Univariate Plots, Outlier Plots.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting EDA for {df_name} ---\")\n",
    "\n",
    "    # Apply memory reduction\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    print(\"\\n--- Basic Information ---\")\n",
    "    print(df.info())\n",
    "    print(\"\\n--- First 5 Rows ---\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n--- Missing Values (Percentage) ---\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values[missing_values > 0] / len(df)) * 100\n",
    "    if not missing_percentage.empty:\n",
    "        print(missing_percentage.sort_values(ascending=False))\n",
    "    else:\n",
    "        print(\"No missing values found.\")\n",
    "\n",
    "    # --- Descriptive Statistics ---\n",
    "    print(\"\\n--- Descriptive Statistics for Numerical Features ---\")\n",
    "    print(df.select_dtypes(include=np.number).describe().transpose()) # Transpose for better readability\n",
    "\n",
    "    # Check if there are any categorical columns before describing them\n",
    "    categorical_cols_for_desc = df.select_dtypes(include='category')\n",
    "    print(\"\\n--- Descriptive Statistics for Categorical Features ---\")\n",
    "    if not categorical_cols_for_desc.empty:\n",
    "        print(categorical_cols_for_desc.describe().transpose())\n",
    "    else:\n",
    "        print(\"No categorical columns found for descriptive statistics.\")\n",
    "\n",
    "\n",
    "    # --- Univariate Analysis: Numerical Features Distribution ---\n",
    "    print(\"\\n--- Univariate Analysis: Numerical Features Distribution ---\")\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns.drop(id_cols, errors='ignore')\n",
    "    if target_col and target_col in numerical_cols:\n",
    "        numerical_cols = numerical_cols.drop(target_col, errors='ignore')\n",
    "\n",
    "    # Plot histograms for a sample of numerical columns to avoid too many plots\n",
    "    n_cols_to_plot = min(len(numerical_cols), 5) # Plot up to 5 random numerical columns\n",
    "    selected_numerical_cols = np.array([]) # Initialize as empty NumPy array\n",
    "    if n_cols_to_plot > 0:\n",
    "        selected_numerical_cols = np.random.choice(numerical_cols, n_cols_to_plot, replace=False)\n",
    "        for col in selected_numerical_cols:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.histplot(df[col].dropna(), kde=True, bins=50)\n",
    "            plt.title(f'Distribution of {col} in {df_name}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No numerical columns to plot (excluding IDs and target).\")\n",
    "\n",
    "    # --- Univariate Analysis: Categorical Features Distribution ---\n",
    "    print(\"\\n--- Univariate Analysis: Categorical Features Distribution ---\")\n",
    "    categorical_cols = df.select_dtypes(include='category').columns\n",
    "\n",
    "    if not categorical_cols.empty:\n",
    "        for col in categorical_cols:\n",
    "            if df[col].nunique() < 50: # Limit to categories with fewer unique values for plotting\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.countplot(y=col, data=df, order=df[col].value_counts().index)\n",
    "                plt.title(f'Distribution of {col} in {df_name}')\n",
    "                plt.xlabel('Count')\n",
    "                plt.ylabel(col)\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"Skipping countplot for {col} in {df_name} due to high number of unique categories ({df[col].nunique()}).\")\n",
    "    else:\n",
    "        print(\"No categorical columns found for univariate analysis.\")\n",
    "\n",
    "    # --- Outlier Detection (using Box Plots for key numerical features) ---\n",
    "    print(\"\\n--- Outlier Detection (using Box Plots) ---\")\n",
    "    outlier_cols_sample = selected_numerical_cols # Use the same sampled numerical columns\n",
    "\n",
    "    # Change .empty to len() == 0 for NumPy array check\n",
    "    if len(outlier_cols_sample) > 0: # Check if there are columns selected\n",
    "        for col in outlier_cols_sample:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.boxplot(x=df[col].dropna())\n",
    "            plt.title(f'Box Plot of {col} for Outlier Detection in {df_name}')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No numerical columns selected for outlier detection plots.\")\n",
    "\n",
    "\n",
    "    # --- Bivariate Analysis (if a target column is specified) ---\n",
    "    if target_col and target_col in df.columns:\n",
    "        print(f\"\\n--- Bivariate Analysis: Relationship with {target_col} ---\")\n",
    "        # Numerical Features vs. TARGET\n",
    "        # Change .empty to len() == 0 for NumPy array check\n",
    "        if len(selected_numerical_cols) > 0:\n",
    "            for col in selected_numerical_cols:\n",
    "                plt.figure(figsize=(8, 5))\n",
    "                sns.boxplot(x=target_col, y=col, data=df)\n",
    "                plt.title(f'{col} vs. {target_col} in {df_name}')\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"No numerical columns to plot against target.\")\n",
    "\n",
    "        # Categorical Features vs. TARGET\n",
    "        if not categorical_cols.empty:\n",
    "            for col in categorical_cols:\n",
    "                if df[col].nunique() < 20: # Limit to categories with fewer unique values\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    sns.barplot(x=col, y=target_col, data=df, ci=None)\n",
    "                    plt.title(f'{target_col} Rate by {col} in {df_name}')\n",
    "                    plt.ylabel(f'{target_col} Rate (TARGET = 1)')\n",
    "                    plt.xticks(rotation=45, ha='right')\n",
    "                    plt.show()\n",
    "                else:\n",
    "                    print(f\"Skipping bar plot for {col} vs {target_col} in {df_name} due to high number of unique categories.\")\n",
    "        else:\n",
    "            print(\"No categorical columns to plot against target.\")\n",
    "\n",
    "    # --- Correlation Heatmap (for key numerical features) ---\n",
    "    print(f\"\\n--- Correlation Heatmap for {df_name} (Sample of Numerical Features) ---\")\n",
    "    # Limit features for correlation to avoid very large heatmaps\n",
    "    corr_df = df.select_dtypes(include=np.number)\n",
    "    # Reduce columns if too many (e.g., take top correlated with target if exists, or a random sample)\n",
    "    if corr_df.shape[1] > 15:\n",
    "        # If target_col exists, pick top correlated. Otherwise, random random.choice(corr_df.columns, 15, replace=False)\n",
    "        if target_col and target_col in corr_df.columns:\n",
    "            top_corr_cols = corr_df.corrwith(corr_df[target_col]).abs().sort_values(ascending=False).index[:15]\n",
    "            corr_df = corr_df[top_corr_cols]\n",
    "        else:\n",
    "            corr_df = corr_df[np.random.choice(corr_df.columns, 15, replace=False)]\n",
    "\n",
    "    if corr_df.shape[1] > 1: # Ensure there's more than one column for correlation\n",
    "        correlation_matrix = corr_df.corr()\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "        plt.title(f'Correlation Matrix of Key Numerical Features in {df_name}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough numerical columns to plot a correlation heatmap.\")\n",
    "\n",
    "    print(f\"--- Finished EDA for {df_name} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a4789-0b3c-4d40-b5d5-413435a8aabd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- EDA for application_train.csv ---\")\n",
    "try:\n",
    "    app_train = pd.read_csv('application_train.csv')\n",
    "    perform_eda(app_train, 'application_train.csv', id_cols=['SK_ID_CURR'], target_col='TARGET')\n",
    "    del app_train # Delete dataframe to free up memory\n",
    "    gc.collect() # Explicitly call garbage collector\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: application_train.csv not found. Please ensure the file is in the correct directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3606ef2-75a1-47c5-97aa-5aa4001893dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- EDA for application_test.csv ---\")\n",
    "try:\n",
    "    app_test = pd.read_csv('application_test.csv')\n",
    "    perform_eda(app_test, 'application_test.csv', id_cols=['SK_ID_CURR']) # No target column\n",
    "    del app_test\n",
    "    gc.collect()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: application_test.csv not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b24d9-c7bb-4f72-a0a2-1d4318873bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- EDA for bureau.csv ---\")\n",
    "try:\n",
    "    bureau = pd.read_csv('bureau.csv')\n",
    "    perform_eda(bureau, 'bureau.csv', id_cols=['SK_ID_CURR', 'SK_ID_BUREAU'])\n",
    "    del bureau\n",
    "    gc.collect()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: bureau.csv not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa90815-b26d-464a-89d0-978a0ff33cd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- EDA for bureau_balance.csv ---\")\n",
    "try:\n",
    "    bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "    perform_eda(bureau_balance, 'bureau_balance.csv', id_cols=['SK_ID_BUREAU'])\n",
    "    del bureau_balance\n",
    "    gc.collect()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: bureau_balance.csv not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b36f1-3975-4a93-885c-4622efb07f8f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- EDA for POS_CASH_balance.csv ---\")\n",
    "try:\n",
    "    pos_cash_balance = pd.read_csv('POS_CASH_balance.csv')\n",
    "    perform_eda(pos_cash_balance, 'POS_CASH_balance.csv', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "    del pos_cash_balance\n",
    "    gc.collect()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: POS_CASH_balance.csv not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6169dbbf-ea1c-452a-a841-73277d5f7ab6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- EDA for credit_card_balance.csv ---\")\n",
    "try:\n",
    "    credit_card_balance = pd.read_csv('credit_card_balance.csv')\n",
    "    perform_eda(credit_card_balance, 'credit_card_balance.csv', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "    del credit_card_balance\n",
    "    gc.collect()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: credit_card_balance.csv not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cc8f8-c1ca-4174-bc62-99102632e0d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- EDA for previous_application.csv ---\")\n",
    "try:\n",
    "    previous_application = pd.read_csv('previous_application.csv')\n",
    "    perform_eda(previous_application, 'previous_application.csv', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "    del previous_application\n",
    "    gc.collect()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: previous_application.csv not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ff46f-a59d-4faa-9be5-ebd14b61c147",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- EDA for installments_payments.csv ---\")\n",
    "try:\n",
    "    installments_payments = pd.read_csv('installments_payments.csv')\n",
    "    perform_eda(installments_payments, 'installments_payments.csv', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "    del installments_payments\n",
    "    gc.collect()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: installments_payments.csv not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d7c23-7de1-4a3d-b742-8e9042ad7f13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- HomeCredit_columns_description.csv ---\")\n",
    "try:\n",
    "    column_descriptions = pd.read_csv('HomeCredit_columns_description.csv', encoding='latin1') # Adjust encoding if necessary\n",
    "    print(\"HomeCredit_columns_description.csv loaded successfully!\")\n",
    "    print(\"\\n--- Sample Column Descriptions ---\")\n",
    "    print(column_descriptions.head())\n",
    "    # You can also filter for specific tables, e.g.,\n",
    "    # print(\"\\n--- Descriptions for application_train.csv columns ---\")\n",
    "    # print(column_descriptions[column_descriptions['Table'] == 'application_train.csv'].head())\n",
    "    del column_descriptions\n",
    "    gc.collect()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: HomeCredit_columns_description.csv not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd2b16-4d2c-4cb5-b7ac-e2e93865e62b",
   "metadata": {},
   "source": [
    "### Preprocessing & feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e304f1b-075e-47ec-902a-d8e302c654bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Import yang benar dan terbaru dari scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer # SimpleImputer sekarang dari sklearn.impute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277d2b0a-497a-44ad-95e5-9a83b8aa66b9",
   "metadata": {},
   "source": [
    "##### Optimasi Memori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3202e20-18fd-462e-af1d-34b3f30b3bc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Fungsi Utility ---\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else: # float types\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cc265-2297-45a5-8555-205c5e96abb8",
   "metadata": {},
   "source": [
    "##### Data cleaning (handle missing values & outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7789e543-07f6-4e33-a953-f5c0c13c2301",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Handle missing value & outliers\n",
    "\n",
    "def handle_specific_outliers(df, df_name):\n",
    "    \"\"\" Handles specific known outliers like DAYS_EMPLOYED anomaly. \"\"\"\n",
    "    if 'DAYS_EMPLOYED' in df.columns:\n",
    "        print(f\"  Handling specific outliers for DAYS_EMPLOYED in {df_name}...\")\n",
    "        df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
    "        df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n",
    "        print(f\"    Created DAYS_EMPLOYED_ANOM and replaced 365243 with NaN.\")\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df, df_name):\n",
    "    \"\"\" Fills missing values and potentially drops columns with too many NaNs. \"\"\"\n",
    "    print(f\"  Handling missing values in {df_name}...\")\n",
    "    # Drop columns with too many missing values (e.g., > 70%)\n",
    "    threshold = 0.7 * len(df)\n",
    "    cols_to_drop = [col for col in df.columns if df[col].isnull().sum() > threshold]\n",
    "    if cols_to_drop:\n",
    "        print(f\"    Dropping {len(cols_to_drop)} columns with more than 70% missing values in {df_name}.\")\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    # Separate numerical and categorical columns for imputation\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    # Impute numerical columns with median, excluding IDs and TARGET\n",
    "    # PASTIKAN 'TARGET' DIKECUALIKAN DARI IMPUTASI NUMERIK\n",
    "    cols_to_impute_num = [col for col in numerical_cols if col not in ['SK_ID_CURR', 'SK_ID_PREV', 'SK_ID_BUREAU', 'TARGET']] # Added SK_ID_BUREAU\n",
    "    if cols_to_impute_num:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        df[cols_to_impute_num] = imputer_num.fit_transform(df[cols_to_impute_num])\n",
    "        print(f\"    Imputed {len(cols_to_impute_num)} numerical columns with median.\")\n",
    "\n",
    "    # Impute categorical columns with mode\n",
    "    if categorical_cols:\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
    "        print(f\"    Imputed {len(categorical_cols)} categorical columns with mode.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_all_numeric_outliers(df, df_name, id_cols=None):\n",
    "    \"\"\" Caps outliers for all numerical columns using IQR method (1% and 99% percentiles). \"\"\"\n",
    "    print(f\"  Handling numerical outliers in {df_name} using 1st and 99th percentile capping...\")\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    if id_cols is None:\n",
    "        id_cols = ['SK_ID_CURR'] # Default IDs\n",
    "    # Exclude ID columns and TARGET from outlier capping\n",
    "    cols_to_cap = [col for col in numerical_cols if col not in id_cols and col != 'TARGET']\n",
    "\n",
    "    for col in cols_to_cap:\n",
    "        # Check if column has enough non-NaN values to calculate percentiles\n",
    "        if df[col].count() > 0:\n",
    "            lower_bound = df[col].quantile(0.01)\n",
    "            upper_bound = df[col].quantile(0.99)\n",
    "            df[col] = np.clip(df[col], lower_bound, upper_bound)\n",
    "    print(f\"    Capped outliers for {len(cols_to_cap)} numerical columns.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00bde47-fb7c-4cf2-9f4b-33ed6787df58",
   "metadata": {},
   "source": [
    "##### Transformasi Fitur (encoding, scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c8063e-4f52-40bf-83be-e7a8df3bb4eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Fungsi Utama Pipeline Transformasi Data ---\n",
    "def process_and_aggregate_data():\n",
    "    # --- Load main application data ---\n",
    "    print(\"\\n--- Loading main application data ---\")\n",
    "    app_train = pd.read_csv('application_train.csv')\n",
    "    app_test = pd.read_csv('application_test.csv')\n",
    "\n",
    "    print(f\"Initial app_train shape: {app_train.shape}\")\n",
    "    print(f\"Initial app_test shape: {app_test.shape}\")\n",
    "\n",
    "    # Tambahkan kolom 'TARGET' ke app_test dengan semua nilai NaN\n",
    "    app_test['TARGET'] = np.nan # KRITIS: Memastikan data test memiliki kolom 'TARGET' dengan NaN\n",
    "\n",
    "    # Gabungkan train dan test\n",
    "    df = pd.concat([app_train, app_test], ignore_index=True)\n",
    "    del app_train, app_test # Hapus dataframe asli untuk menghemat memori\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Combined df shape after concat: {df.shape}\")\n",
    "\n",
    "    # Process combined application data\n",
    "    df = reduce_mem_usage(df)\n",
    "    df = handle_specific_outliers(df, 'Application Data')\n",
    "    df = handle_missing_values(df, 'Application Data')\n",
    "    df = handle_all_numeric_outliers(df, 'Application Data', id_cols=['SK_ID_CURR', 'TARGET'])\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Process Bureau and Bureau Balance ---\n",
    "    print(\"\\n--- Processing Bureau and Bureau Balance ---\")\n",
    "    bureau = pd.read_csv('bureau.csv')\n",
    "    bureau = reduce_mem_usage(bureau)\n",
    "    bureau = handle_missing_values(bureau, 'Bureau Data')\n",
    "    bureau = handle_all_numeric_outliers(bureau, 'Bureau Data', id_cols=['SK_ID_CURR', 'SK_ID_BUREAU'])\n",
    "\n",
    "    bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "    bureau_balance = reduce_mem_usage(bureau_balance)\n",
    "    bureau_balance = handle_missing_values(bureau_balance, 'Bureau Balance Data') # No specific outliers for BB\n",
    "\n",
    "    print(\"  Aggregating bureau_balance data...\")\n",
    "    # Debugging: Periksa kondisi bureau_balance sebelum agregasi\n",
    "    print(f\"  Shape of bureau_balance AFTER cleaning: {bureau_balance.shape}\")\n",
    "    print(f\"  Unique SK_ID_BUREAU in bureau_balance: {bureau_balance['SK_ID_BUREAU'].nunique()}\")\n",
    "    print(f\"  Is SK_ID_BUREAU present in bureau_balance? {'SK_ID_BUREAU' in bureau_balance.columns}\")\n",
    "    print(f\"  Data types of bureau_balance: \\n{bureau_balance.dtypes}\")\n",
    "    print(f\"  Categorical columns in bureau_balance: {bureau_balance.select_dtypes(include='category').columns.tolist()}\")\n",
    "    print(f\"  Numerical columns in bureau_balance: {bureau_balance.select_dtypes(include=np.number).columns.tolist()}\")\n",
    "\n",
    "    bb_numerical_cols = bureau_balance.select_dtypes(include=np.number).columns.drop(['SK_ID_BUREAU'], errors='ignore').tolist()\n",
    "    bb_categorical_cols = bureau_balance.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    bb_agg_dict = {}\n",
    "    # Agregasi kolom numerik (MONTHS_BALANCE adalah yang utama di sini)\n",
    "    for col in bb_numerical_cols:\n",
    "        bb_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'count', 'nunique']\n",
    "\n",
    "    # Agregasi kolom kategorikal (STATUS adalah yang utama di sini)\n",
    "    for col in bb_categorical_cols:\n",
    "        bb_agg_dict[col] = ['count', 'nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    # Periksa apakah dictionary agregasi kosong. Penting agar .agg() tidak error.\n",
    "    if not bb_agg_dict:\n",
    "        print(\"WARNING: Aggregation dictionary for bureau_balance is empty. This might indicate no relevant columns were found.\")\n",
    "        # Jika Anda yakin tidak ada kolom yang bisa diagregasi, Anda bisa melewati merge ini\n",
    "        # Untuk saat ini, kita akan teruskan, tapi jika ini terus error, ini adalah masalahnya\n",
    "        bureau_balance_agg = pd.DataFrame({'SK_ID_BUREAU': bureau_balance['SK_ID_BUREAU'].unique()}) # Buat DF kosong dengan SK_ID_BUREAU saja\n",
    "    else:\n",
    "        bureau_balance_agg = bureau_balance.groupby('SK_ID_BUREAU').agg(bb_agg_dict).reset_index()\n",
    "\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_BUREAU']\n",
    "    if not bureau_balance_agg.empty: # Only process if DataFrame is not empty\n",
    "        for col_tuple in bureau_balance_agg.columns.drop('SK_ID_BUREAU'):\n",
    "            if isinstance(col_tuple, tuple):\n",
    "                if col_tuple[1] == '<lambda>':\n",
    "                    new_cols.append(f'BB_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "                else:\n",
    "                    new_cols.append(f'BB_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "            else:\n",
    "                new_cols.append(f'BB_{str(col_tuple).upper()}') # Should not happen if tuples are always created\n",
    "\n",
    "        bureau_balance_agg.columns = new_cols\n",
    "    del bureau_balance\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated bureau_balance data. Shape: {bureau_balance_agg.shape}\")\n",
    "\n",
    "    # Merge aggregated bureau_balance to bureau\n",
    "    bureau = bureau.merge(bureau_balance_agg, on='SK_ID_BUREAU', how='left')\n",
    "    del bureau_balance_agg\n",
    "    gc.collect()\n",
    "    print(f\"  Merged bureau_balance to bureau. Bureau shape: {bureau.shape}\")\n",
    "\n",
    "    print(\"  Aggregating bureau data...\")\n",
    "    # Aggregate bureau data to SK_ID_CURR level\n",
    "    bureau_numerical_cols = bureau.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_BUREAU'], errors='ignore').tolist()\n",
    "    bureau_categorical_cols = bureau.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    bureau_agg_dict = {}\n",
    "    for col in bureau_numerical_cols:\n",
    "        bureau_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median', 'first', 'last']\n",
    "    for col in bureau_categorical_cols:\n",
    "        bureau_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg(bureau_agg_dict).reset_index()\n",
    "\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in bureau_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'BUREAU_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'BUREAU_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "             new_cols.append(f'BUREAU_{str(col_tuple).upper()}')\n",
    "\n",
    "    bureau_agg.columns = new_cols\n",
    "    del bureau\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated bureau data. Shape: {bureau_agg.shape}\")\n",
    "\n",
    "    # Merge aggregated bureau data to main df\n",
    "    df = df.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "    del bureau_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged bureau data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "    # --- Process Previous Application ---\n",
    "    print(\"\\n--- Processing Previous Application ---\")\n",
    "    previous_application = pd.read_csv('previous_application.csv')\n",
    "    previous_application = reduce_mem_usage(previous_application)\n",
    "    previous_application = handle_missing_values(previous_application, 'Previous Application Data')\n",
    "    previous_application = handle_all_numeric_outliers(previous_application, 'Previous Application Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating previous_application data...\")\n",
    "    # Aggregate previous_application\n",
    "    prev_num_cols = previous_application.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    prev_cat_cols = previous_application.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    prev_agg_dict = {}\n",
    "    for col in prev_num_cols:\n",
    "        prev_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in prev_cat_cols:\n",
    "        prev_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    previous_application_agg = previous_application.groupby('SK_ID_CURR').agg(prev_agg_dict).reset_index()\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in previous_application_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'PREV_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'PREV_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "            new_cols.append(f'PREV_{str(col_tuple).upper()}')\n",
    "\n",
    "    previous_application_agg.columns = new_cols\n",
    "    del previous_application\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated previous_application data. Shape: {previous_application_agg.shape}\")\n",
    "\n",
    "    df = df.merge(previous_application_agg, on='SK_ID_CURR', how='left')\n",
    "    del previous_application_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged previous_application data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "\n",
    "    # --- Process POS_CASH_balance ---\n",
    "    print(\"\\n--- Processing POS_CASH_balance ---\")\n",
    "    pos_cash = pd.read_csv('POS_CASH_balance.csv')\n",
    "    pos_cash = reduce_mem_usage(pos_cash)\n",
    "    pos_cash = handle_missing_values(pos_cash, 'POS_CASH_balance Data')\n",
    "    pos_cash = handle_all_numeric_outliers(pos_cash, 'POS_CASH_balance Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating POS_CASH_balance data...\")\n",
    "    # Aggregate POS_CASH_balance\n",
    "    pos_cash_num_cols = pos_cash.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    pos_cash_cat_cols = pos_cash.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    pos_cash_agg_dict = {}\n",
    "    for col in pos_cash_num_cols:\n",
    "        pos_cash_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in pos_cash_cat_cols:\n",
    "        pos_cash_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    pos_cash_agg = pos_cash.groupby('SK_ID_CURR').agg(pos_cash_agg_dict).reset_index()\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in pos_cash_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'POS_CASH_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'POS_CASH_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "            new_cols.append(f'POS_CASH_{str(col_tuple).upper()}')\n",
    "\n",
    "    pos_cash_agg.columns = new_cols\n",
    "    del pos_cash\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated POS_CASH_balance data. Shape: {pos_cash_agg.shape}\")\n",
    "\n",
    "    df = df.merge(pos_cash_agg, on='SK_ID_CURR', how='left')\n",
    "    del pos_cash_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged POS_CASH_balance data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "    # --- Process Credit Card Balance ---\n",
    "    print(\"\\n--- Processing Credit Card Balance ---\")\n",
    "    credit_card = pd.read_csv('credit_card_balance.csv')\n",
    "    credit_card = reduce_mem_usage(credit_card)\n",
    "    credit_card = handle_missing_values(credit_card, 'Credit Card Balance Data')\n",
    "    credit_card = handle_all_numeric_outliers(credit_card, 'Credit Card Balance Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating credit_card_balance data...\")\n",
    "\n",
    "    credit_card_numerical_cols = credit_card.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    credit_card_categorical_cols = credit_card.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    credit_card_agg_dict = {}\n",
    "    for col in credit_card_numerical_cols:\n",
    "        credit_card_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in credit_card_categorical_cols:\n",
    "        credit_card_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    credit_card_agg = credit_card.groupby('SK_ID_CURR').agg(credit_card_agg_dict).reset_index()\n",
    "\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in credit_card_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'CC_BAL_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'CC_BAL_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "             new_cols.append(f'CC_BAL_{str(col_tuple).upper()}')\n",
    "\n",
    "    credit_card_agg.columns = new_cols\n",
    "    del credit_card\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated credit_card_balance data. Shape: {credit_card_agg.shape}\")\n",
    "\n",
    "    df = df.merge(credit_card_agg, on='SK_ID_CURR', how='left')\n",
    "    del credit_card_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged credit_card_balance data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "\n",
    "    # --- Process Installments Payments ---\n",
    "    print(\"\\n--- Processing Installments Payments ---\")\n",
    "    installments_payments = pd.read_csv('installments_payments.csv')\n",
    "    installments_payments = reduce_mem_usage(installments_payments)\n",
    "    installments_payments = handle_missing_values(installments_payments, 'Installments Payments Data')\n",
    "    installments_payments = handle_all_numeric_outliers(installments_payments, 'Installments Payments Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating installments_payments data...\")\n",
    "    # Aggregate installments_payments\n",
    "    inst_num_cols = installments_payments.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    inst_cat_cols = installments_payments.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    inst_agg_dict = {}\n",
    "    for col in inst_num_cols:\n",
    "        inst_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in inst_cat_cols:\n",
    "        inst_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    installments_payments_agg = installments_payments.groupby('SK_ID_CURR').agg(inst_agg_dict).reset_index()\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in installments_payments_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'INSTAL_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'INSTAL_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "            new_cols.append(f'INSTAL_{str(col_tuple).upper()}')\n",
    "\n",
    "    installments_payments_agg.columns = new_cols\n",
    "    del installments_payments\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated installments_payments data. Shape: {installments_payments_agg.shape}\")\n",
    "\n",
    "    df = df.merge(installments_payments_agg, on='SK_ID_CURR', how='left')\n",
    "    del installments_payments_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged installments_payments data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "    # --- Final Data Transformation on the Combined DataFrame ---\n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"Starting Final Data Transformation (Encoding & Scaling)\")\n",
    "    print(\"=======================================================\")\n",
    "\n",
    "    # Handle categorical features (One-Hot Encoding for multi-valued categories)\n",
    "    # and Label Encoding for binary categories\n",
    "    print(\"  Encoding categorical features...\")\n",
    "    # Ambil ulang kolom kategorikal setelah semua merge\n",
    "    all_categorical_cols = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    # Pisahkan kolom biner dan multi-kelas\n",
    "    binary_cat_cols = [col for col in all_categorical_cols if df[col].nunique() == 2]\n",
    "    multi_cat_cols = [col for col in all_categorical_cols if df[col].nunique() > 2]\n",
    "\n",
    "    # Label Encode fitur kategorikal biner\n",
    "    for col in binary_cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        print(f\"    Label encoded: {col}\")\n",
    "\n",
    "    # One-Hot Encode fitur kategorikal multi-kelas\n",
    "    if multi_cat_cols:\n",
    "        print(f\"    One-Hot Encoding {len(multi_cat_cols)} multi-class categorical features...\")\n",
    "        df = pd.get_dummies(df, columns=multi_cat_cols, dummy_na=False)\n",
    "        print(f\"    DataFrame shape after One-Hot Encoding: {df.shape}\")\n",
    "    else:\n",
    "        print(\"    No multi-class categorical features to One-Hot Encode.\")\n",
    "    gc.collect()\n",
    "\n",
    "    # --- STRATEGI PERBAIKAN: MENGISI SEMUA NaN DENGAN 0 ---\n",
    "    # Ini akan sangat mempercepat proses dibanding SimpleImputer untuk banyak kolom.\n",
    "    print(\"  Handling remaining missing values with 0 and infinite values...\")\n",
    "\n",
    "    # Identifikasi kolom numerik yang mungkin masih memiliki NaN setelah semua merge dan encoding\n",
    "    # Exclude SK_ID_CURR and TARGET from this operation\n",
    "    numerical_cols_for_final_fill = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['SK_ID_CURR', 'TARGET']]\n",
    "\n",
    "    # Isi NaN dengan 0 untuk kolom-kolom ini\n",
    "    df[numerical_cols_for_final_fill] = df[numerical_cols_for_final_fill].fillna(0)\n",
    "    print(f\"    Filled all remaining numerical NaNs with 0 for {len(numerical_cols_for_final_fill)} columns.\")\n",
    "\n",
    "    # Ganti nilai infinite dengan 0\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    print(\"    Replaced infinite values with 0.\")\n",
    "\n",
    "    # Optional: Verifikasi apakah masih ada NaN yang tersisa\n",
    "    if df.isnull().any().any():\n",
    "        print(\"WARNING: Some NaNs still exist after final fillna(0). Check these columns:\")\n",
    "        print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "\n",
    "    print(f\"  Final DataFrame shape after all preprocessing (before splitting): {df.shape}\")\n",
    "\n",
    "     # --- Tambahkan ini ---\n",
    "    print(\"  Re-optimizing memory usage before splitting and scaling...\")\n",
    "    df = reduce_mem_usage(df) # Jalankan lagi!\n",
    "    gc.collect()\n",
    "    # --- End Tambahan ---\n",
    "    \n",
    "    # Separate train and test data\n",
    "    train_df = df[df['TARGET'].notna()].copy()\n",
    "    test_df = df[df['TARGET'].isna()].drop(columns=['TARGET']).copy()\n",
    "\n",
    "    print(f\"\\nTrain DataFrame shape AFTER SPLIT: {train_df.shape}\")\n",
    "    print(f\"Test DataFrame shape AFTER SPLIT: {test_df.shape}\")\n",
    "\n",
    "    # Pastikan test_df memiliki SK_ID_CURR untuk submission\n",
    "    if 'SK_ID_CURR' not in test_df.columns:\n",
    "        print(\"WARNING: SK_ID_CURR not found in test_df after splitting. This might cause submission issues.\")\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Apply StandardScaler to numerical features\n",
    "    print(\"  Applying StandardScaler to numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    # Identifikasi kolom numerik umum yang akan di-scale\n",
    "    common_numerical_cols = [col for col in train_df.columns if col in test_df.columns and col not in ['SK_ID_CURR', 'TARGET'] and pd.api.types.is_numeric_dtype(train_df[col])]\n",
    "\n",
    "    train_df[common_numerical_cols] = scaler.fit_transform(train_df[common_numerical_cols].astype(np.float32))\n",
    "    test_df[common_numerical_cols] = scaler.transform(test_df[common_numerical_cols].astype(np.float32))\n",
    "    print(f\"  Scaled {len(common_numerical_cols)} numerical features.\")\n",
    "    gc.collect() # Panggil gc.collect() setelah operasi besar\n",
    "    \n",
    "    print(f\"  Scaled {len(common_numerical_cols)} numerical features.\")\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d691bd2c-6b0a-459e-b965-6ca0ab18eb5a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading main application data ---\n",
      "Initial app_train shape: (307511, 122)\n",
      "Initial app_test shape: (48744, 121)\n",
      "Combined df shape after concat: (356255, 122)\n",
      "Memory usage of dataframe is 331.60 MB\n",
      "Memory usage after optimization is: 69.32 MB\n",
      "Decreased by 79.1%\n",
      "  Handling specific outliers for DAYS_EMPLOYED in Application Data...\n",
      "    Created DAYS_EMPLOYED_ANOM and replaced 365243 with NaN.\n",
      "  Handling missing values in Application Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\581809730.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\581809730.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Imputed 104 numerical columns with median.\n",
      "    Imputed 16 categorical columns with mode.\n",
      "  Handling numerical outliers in Application Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 104 numerical columns.\n",
      "\n",
      "--- Processing Bureau and Bureau Balance ---\n",
      "Memory usage of dataframe is 222.62 MB\n",
      "Memory usage after optimization is: 78.57 MB\n",
      "Decreased by 64.7%\n",
      "  Handling missing values in Bureau Data...\n",
      "    Dropping 1 columns with more than 70% missing values in Bureau Data.\n",
      "    Imputed 11 numerical columns with median.\n",
      "    Imputed 3 categorical columns with mode.\n",
      "  Handling numerical outliers in Bureau Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 11 numerical columns.\n",
      "Memory usage of dataframe is 624.85 MB\n",
      "Memory usage after optimization is: 156.21 MB\n",
      "Decreased by 75.0%\n",
      "  Handling missing values in Bureau Balance Data...\n",
      "    Imputed 1 numerical columns with median.\n",
      "    Imputed 1 categorical columns with mode.\n",
      "  Aggregating bureau_balance data...\n",
      "  Shape of bureau_balance AFTER cleaning: (27299925, 3)\n",
      "  Unique SK_ID_BUREAU in bureau_balance: 817395\n",
      "  Is SK_ID_BUREAU present in bureau_balance? True\n",
      "  Data types of bureau_balance: \n",
      "SK_ID_BUREAU        int32\n",
      "MONTHS_BALANCE    float64\n",
      "STATUS             object\n",
      "dtype: object\n",
      "  Categorical columns in bureau_balance: []\n",
      "  Numerical columns in bureau_balance: ['SK_ID_BUREAU', 'MONTHS_BALANCE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:72: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in bureau_balance_agg.columns.drop('SK_ID_BUREAU'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated bureau_balance data. Shape: (817395, 7)\n",
      "  Merged bureau_balance to bureau. Bureau shape: (1716428, 22)\n",
      "  Aggregating bureau data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:107: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in bureau_agg.columns.drop('SK_ID_CURR'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated bureau data. Shape: (305811, 154)\n",
      "Merged bureau data to main df. Main df shape: (356255, 276)\n",
      "\n",
      "--- Processing Previous Application ---\n",
      "Memory usage of dataframe is 471.48 MB\n",
      "Memory usage after optimization is: 130.62 MB\n",
      "Decreased by 72.3%\n",
      "  Handling missing values in Previous Application Data...\n",
      "    Dropping 2 columns with more than 70% missing values in Previous Application Data.\n",
      "    Imputed 17 numerical columns with median.\n",
      "    Imputed 16 categorical columns with mode.\n",
      "  Handling numerical outliers in Previous Application Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 17 numerical columns.\n",
      "  Aggregating previous_application data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:148: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in previous_application_agg.columns.drop('SK_ID_CURR'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated previous_application data. Shape: (338857, 120)\n",
      "Merged previous_application data to main df. Main df shape: (356255, 395)\n",
      "\n",
      "--- Processing POS_CASH_balance ---\n",
      "Memory usage of dataframe is 610.43 MB\n",
      "Memory usage after optimization is: 171.69 MB\n",
      "Decreased by 71.9%\n",
      "  Handling missing values in POS_CASH_balance Data...\n",
      "    Imputed 5 numerical columns with median.\n",
      "    Imputed 1 categorical columns with mode.\n",
      "  Handling numerical outliers in POS_CASH_balance Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 5 numerical columns.\n",
      "  Aggregating POS_CASH_balance data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:189: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in pos_cash_agg.columns.drop('SK_ID_CURR'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated POS_CASH_balance data. Shape: (337252, 36)\n",
      "Merged POS_CASH_balance data to main df. Main df shape: (356255, 430)\n",
      "\n",
      "--- Processing Credit Card Balance ---\n",
      "Memory usage of dataframe is 673.88 MB\n",
      "Memory usage after optimization is: 263.69 MB\n",
      "Decreased by 60.9%\n",
      "  Handling missing values in Credit Card Balance Data...\n",
      "    Imputed 20 numerical columns with median.\n",
      "    Imputed 1 categorical columns with mode.\n",
      "  Handling numerical outliers in Credit Card Balance Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 20 numerical columns.\n",
      "  Aggregating credit_card_balance data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:229: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in credit_card_agg.columns.drop('SK_ID_CURR'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated credit_card_balance data. Shape: (103558, 141)\n",
      "Merged credit_card_balance data to main df. Main df shape: (356255, 570)\n",
      "\n",
      "--- Processing Installments Payments ---\n",
      "Memory usage of dataframe is 830.41 MB\n",
      "Memory usage after optimization is: 311.40 MB\n",
      "Decreased by 62.5%\n",
      "  Handling missing values in Installments Payments Data...\n",
      "    Imputed 6 numerical columns with median.\n",
      "  Handling numerical outliers in Installments Payments Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 6 numerical columns.\n",
      "  Aggregating installments_payments data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:270: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in installments_payments_agg.columns.drop('SK_ID_CURR'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated installments_payments data. Shape: (339587, 43)\n",
      "Merged installments_payments data to main df. Main df shape: (356255, 612)\n",
      "\n",
      "=======================================================\n",
      "Starting Final Data Transformation (Encoding & Scaling)\n",
      "=======================================================\n",
      "  Encoding categorical features...\n",
      "    No multi-class categorical features to One-Hot Encode.\n",
      "  Handling remaining missing values with 0 and infinite values...\n",
      "    Filled all remaining numerical NaNs with 0 for 594 columns.\n",
      "    Replaced infinite values with 0.\n",
      "WARNING: Some NaNs still exist after final fillna(0). Check these columns:\n",
      "TARGET    48744\n",
      "dtype: int64\n",
      "  Final DataFrame shape after all preprocessing (before splitting): (356255, 612)\n",
      "  Re-optimizing memory usage before splitting and scaling...\n",
      "Memory usage of dataframe is 1573.39 MB\n",
      "Memory usage after optimization is: 519.83 MB\n",
      "Decreased by 67.0%\n",
      "\n",
      "Train DataFrame shape AFTER SPLIT: (307511, 612)\n",
      "Test DataFrame shape AFTER SPLIT: (48744, 611)\n",
      "  Applying StandardScaler to numerical features...\n",
      "  Scaled 594 numerical features.\n",
      "  Scaled 594 numerical features.\n",
      "\n",
      "--- Transformed Train Data Head (Sample) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1458: RuntimeWarning: overflow encountered in cast\n",
      "  has_large_values = (abs_vals > 1e6).any()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
      "0      100002     1.0         Cash loans           M            N   \n",
      "1      100003     0.0         Cash loans           F            N   \n",
      "2      100004     0.0    Revolving loans           M            Y   \n",
      "3      100006     0.0         Cash loans           F            N   \n",
      "4      100007     0.0         Cash loans           M            N   \n",
      "\n",
      "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
      "0               Y     -0.586054          0.434759   -0.484740    -0.164808   \n",
      "1               N     -0.586054          1.243180    1.780900     0.638160   \n",
      "2               Y     -0.586054         -1.182084   -1.178548    -1.475380   \n",
      "3               Y     -0.586054         -0.373663   -0.724650     0.199222   \n",
      "4               Y     -0.586054         -0.535347   -0.212930    -0.371792   \n",
      "\n",
      "   ...  INSTAL_AMT_INSTALMENT_STD INSTAL_AMT_INSTALMENT_COUNT  \\\n",
      "0  ...                  -0.122802                   -0.460054   \n",
      "1  ...                   2.601186                   -0.312403   \n",
      "2  ...                  -0.684826                   -0.853791   \n",
      "3  ...                   1.737069                   -0.533880   \n",
      "4  ...                  -0.298690                    0.696548   \n",
      "\n",
      "  INSTAL_AMT_INSTALMENT_MEDIAN INSTAL_AMT_PAYMENT_MIN INSTAL_AMT_PAYMENT_MAX  \\\n",
      "0                    -0.230117               0.504224              -0.185409   \n",
      "1                     4.456307               0.209558               1.696012   \n",
      "2                    -0.561817               0.060936              -0.826068   \n",
      "3                     1.454199              -0.266229               1.696012   \n",
      "4                     0.347841              -0.548413              -0.643681   \n",
      "\n",
      "  INSTAL_AMT_PAYMENT_MEAN  INSTAL_AMT_PAYMENT_SUM  INSTAL_AMT_PAYMENT_STD  \\\n",
      "0               -0.253276               -0.471933               -0.233453   \n",
      "1                2.546907                1.003667                2.227077   \n",
      "2               -0.583196               -0.759888               -0.696617   \n",
      "3                1.171473               -0.074561                1.796436   \n",
      "4               -0.204867                0.379581               -0.365581   \n",
      "\n",
      "   INSTAL_AMT_PAYMENT_COUNT  INSTAL_AMT_PAYMENT_MEDIAN  \n",
      "0                 -0.460054                  -0.200059  \n",
      "1                 -0.312403                   4.277330  \n",
      "2                 -0.853791                  -0.516964  \n",
      "3                 -0.533880                   1.409129  \n",
      "4                  0.696548                   0.352119  \n",
      "\n",
      "[5 rows x 612 columns]\n",
      "\n",
      "--- Transformed Test Data Head (Sample) ---\n",
      "        SK_ID_CURR NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
      "307511      100001         Cash loans           F            N   \n",
      "307512      100005         Cash loans           M            N   \n",
      "307513      100013         Cash loans           M            Y   \n",
      "307514      100028         Cash loans           F            N   \n",
      "307515      100038         Cash loans           M            Y   \n",
      "\n",
      "       FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
      "307511               Y     -0.586054         -0.373663   -0.070386   \n",
      "307512               Y     -0.586054         -0.804821   -0.954341   \n",
      "307513               Y     -0.586054          0.434759    0.170927   \n",
      "307514               Y      2.242382          1.782127    2.499998   \n",
      "307515               N      0.828164          0.165285    0.074457   \n",
      "\n",
      "        AMT_ANNUITY  AMT_GOODS_PRICE  ... INSTAL_AMT_INSTALMENT_STD  \\\n",
      "307511    -0.467070        -0.239637  ...                 -0.520131   \n",
      "307512    -0.700010        -0.986295  ...                 -0.583595   \n",
      "307513     3.126244         0.258135  ...                  0.623563   \n",
      "307514     1.610658         2.871439  ...                 -0.530845   \n",
      "307515     0.373023         0.245691  ...                 -0.924989   \n",
      "\n",
      "       INSTAL_AMT_INSTALMENT_COUNT INSTAL_AMT_INSTALMENT_MEDIAN  \\\n",
      "307511                   -0.755356                    -0.679039   \n",
      "307512                   -0.706139                    -0.608154   \n",
      "307513                    2.886708                    -0.922281   \n",
      "307514                    1.853149                    -0.515801   \n",
      "307515                   -0.632314                    -0.072651   \n",
      "\n",
      "       INSTAL_AMT_PAYMENT_MIN INSTAL_AMT_PAYMENT_MAX  INSTAL_AMT_PAYMENT_MEAN  \\\n",
      "307511              -0.099127              -0.723250                -0.672717   \n",
      "307512              -0.000989              -0.719357                -0.646469   \n",
      "307513              -0.548141               1.696012                -0.473293   \n",
      "307514              -0.548413              -0.397937                -0.785686   \n",
      "307515               0.714304              -0.818133                -0.287200   \n",
      "\n",
      "        INSTAL_AMT_PAYMENT_SUM  INSTAL_AMT_PAYMENT_STD  \\\n",
      "307511               -0.730985               -0.560892   \n",
      "307512               -0.709257               -0.613193   \n",
      "307513                1.140662                0.478116   \n",
      "307514               -0.076010               -0.582905   \n",
      "307515               -0.597404               -0.894536   \n",
      "\n",
      "        INSTAL_AMT_PAYMENT_COUNT  INSTAL_AMT_PAYMENT_MEDIAN  \n",
      "307511                 -0.755356                  -0.628958  \n",
      "307512                 -0.706139                  -0.561234  \n",
      "307513                  2.886708                  -0.896865  \n",
      "307514                  1.853149                  -0.574490  \n",
      "307515                 -0.632314                  -0.049617  \n",
      "\n",
      "[5 rows x 611 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Cara Memanggil Fungsi (Uncomment untuk menjalankan) ---\n",
    "train_transformed, test_transformed = process_and_aggregate_data()\n",
    "\n",
    "# # Tampilkan hasil (opsional, untuk verifikasi)\n",
    "print(\"\\n--- Transformed Train Data Head (Sample) ---\")\n",
    "print(train_transformed.head())\n",
    "print(\"\\n--- Transformed Test Data Head (Sample) ---\")\n",
    "print(test_transformed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81c5bfc-2f3d-4fd0-9e44-7d710876be36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Menyimpan\n",
    "train_transformed.to_csv('train_transformed.csv', index=False)\n",
    "test_transformed.to_csv('test_transformed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c1e527-6a93-4dbd-bd3a-a2310d72fcb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Memuat\n",
    "train_transformed = pd.read_csv('train_transformed.csv')\n",
    "test_transformed = pd.read_csv('test_transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d783c27-59cc-4d9d-ae65-3e946bdcd242",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_transformed shape: (307511, 612)\n",
      "Loaded test_transformed shape: (48744, 611)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded train_transformed shape: {train_transformed.shape}\")\n",
    "print(f\"Loaded test_transformed shape: {test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63bfeb-8597-470e-b39d-ea5f09f8d9f7",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b600f55d-4aa4-4e4c-b879-6722aef35dc3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Mempersiapkan Fitur dan Target ---\n",
      "X_train initial shape: (307511, 610)\n",
      "y_train shape: (307511,)\n",
      "X_test initial shape: (48744, 610)\n"
     ]
    }
   ],
   "source": [
    "# --- Definisikan Fitur (X) dan Target (y) ---\n",
    "print(\"\\n--- Mempersiapkan Fitur dan Target ---\")\n",
    "\n",
    "# Ambil semua nama kolom yang mungkin menjadi fitur\n",
    "base_feature_cols = [col for col in train_transformed.columns if col not in ['SK_ID_CURR', 'TARGET']]\n",
    "\n",
    "X_train = train_transformed[base_feature_cols]\n",
    "y_train = train_transformed['TARGET']\n",
    "\n",
    "# Pastikan X_test hanya memiliki kolom yang sama dengan X_train\n",
    "X_test = test_transformed[base_feature_cols]\n",
    "\n",
    "print(f\"X_train initial shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test initial shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6afef8-37c7-4493-a1f7-288161b11821",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Memfilter hanya kolom numerik untuk pelatihan model ---\n",
      "X_train shape setelah filter numerik: (307511, 594)\n",
      "X_test shape setelah filter numerik: (48744, 594)\n"
     ]
    }
   ],
   "source": [
    "# --- PENTING: FILTER HANYA KOLOM NUMERIK ---\n",
    "# Ini adalah langkah kunci untuk mengatasi \"ValueError: could not convert string to float\"\n",
    "print(\"\\n--- Memfilter hanya kolom numerik untuk pelatihan model ---\")\n",
    "numeric_cols_train = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols_test = X_test.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Pastikan set kolom numerik sama antara train dan test setelah filter\n",
    "common_numeric_cols = list(set(numeric_cols_train) & set(numeric_cols_test))\n",
    "\n",
    "X_train = X_train[common_numeric_cols]\n",
    "X_test = X_test[common_numeric_cols]\n",
    "\n",
    "print(f\"X_train shape setelah filter numerik: {X_train.shape}\")\n",
    "print(f\"X_test shape setelah filter numerik: {X_test.shape}\")\n",
    "\n",
    "# Jika ada kolom yang hilang setelah filter, identifikasi mereka:\n",
    "missing_in_test_after_filter = set(X_train.columns) - set(X_test.columns)\n",
    "if missing_in_test_after_filter:\n",
    "    print(f\"Peringatan: Kolom-kolom ini ada di X_train tetapi tidak di X_test setelah filter numerik: {missing_in_test_after_filter}\")\n",
    "    # Anda bisa memutuskan untuk drop dari X_train atau mengisi dengan 0 di X_test jika memang harus ada\n",
    "    # Untuk LogisticRegression, pastikan kolom yang sama persis.\n",
    "    # Namun, pendekatan common_numeric_cols di atas sudah mengatasinya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4369d795-ac7a-4a71-876d-963faa78168d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Mengoptimalkan Tipe Data (float32) untuk X_train dan X_test ---\n",
      "Tipe data dioptimalkan.\n"
     ]
    }
   ],
   "source": [
    "# --- Optimasi Tipe Data Akhir Sebelum Tuning ---\n",
    "\n",
    "# Pastikan semua kolom numerik adalah float32 untuk menghemat memori\n",
    "print(\"\\n--- Mengoptimalkan Tipe Data (float32) untuk X_train dan X_test ---\")\n",
    "for col in X_train.columns:\n",
    "    X_train[col] = X_train[col].astype(np.float32)\n",
    "    X_test[col] = X_test[col].astype(np.float32) # Terapkan juga ke X_test\n",
    "\n",
    "gc.collect()\n",
    "print(\"Tipe data dioptimalkan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77590965-77e4-40aa-8f2e-00d7343916a3",
   "metadata": {},
   "source": [
    "##### Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92144957-7d09-4a59-86e3-4438e344d1fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Mengambil Sampel 10.0% Data untuk Hyperparameter Tuning ---\n",
      "Shape X_train_sample untuk tuning: (30751, 594)\n"
     ]
    }
   ],
   "source": [
    "# --- STRATEGI KRITIS: Mengambil Sampel Data untuk Tuning ---\n",
    "# Ini adalah langkah paling penting untuk mencegah laptop mati tiba-tiba.\n",
    "# Kita akan melakukan tuning hyperparameter pada subset data yang lebih kecil.\n",
    "\n",
    "sample_fraction_for_tuning = 0.1 # Coba 10% dari data (sesuaikan: 0.05 untuk 5% jika 10% masih berat)\n",
    "print(f\"\\n--- Mengambil Sampel {sample_fraction_for_tuning*100}% Data untuk Hyperparameter Tuning ---\")\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(\n",
    "    X_train, y_train, train_size=sample_fraction_for_tuning, stratify=y_train, random_state=42\n",
    ")\n",
    "print(f\"Shape X_train_sample untuk tuning: {X_train_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeca4acb-ba32-4ee4-940b-8fab0be919fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds didefinisikan dengan n_splits=3\n"
     ]
    }
   ],
   "source": [
    "# --- Definisikan Folds untuk Cross-Validation (akan digunakan oleh RandomizedSearchCV) ---\n",
    "\n",
    "# Menggunakan 3 folds untuk mempercepat proses tuning, bisa tingkatkan ke 5 jika laptop mampu\n",
    "folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "print(f\"Folds didefinisikan dengan n_splits={folds.n_splits}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c0fbda-f2e3-47c6-8e2e-ca332d3a26d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Inisialisasi Model Logistic Regression Dasar ---\n",
    "model_lr_base = LogisticRegression(max_iter=2000, # Tingkatkan iterasi untuk konvergensi yang lebih baik\n",
    "                                   class_weight='balanced', # Penting untuk data tidak seimbang\n",
    "                                   random_state=42,\n",
    "                                   n_jobs=1) # Tetap 1 untuk mencegah MemoryError saat paralel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c51af6c0-be0e-417e-bb84-eb89ba4c3e67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Tentukan Distribusi Parameter untuk RandomizedSearchCV ---\n",
    "param_distributions = {\n",
    "    'C': loguniform(1e-4, 1e2), # Coba C dalam skala logaritmik (dari 0.0001 hingga 100)\n",
    "    'solver': ['lbfgs', 'saga'], # 'lbfgs' adalah default dan baik, 'saga' lebih fleksibel untuk penalty\n",
    "    'penalty': ['l2'] # 'l2' untuk 'lbfgs'. Jika ingin 'l1' atau 'elasticnet', 'solver' harus 'saga'.\n",
    "                      # Contoh: Jika solver = ['saga'], maka penalty = ['l1', 'l2', 'elasticnet']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "183aff33-247a-4043-b82d-2ecdb4fcea32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Memulai RandomizedSearchCV dengan 5 iterasi ---\n",
      "Ini akan menggunakan data sampel dan n_jobs=1 untuk menghemat memori.\n",
      "Menjalankan random_search.fit(). Ini mungkin memakan waktu...\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END ...C=0.017670169402947963, penalty=l2, solver=lbfgs; total time=   6.4s\n",
      "[CV] END ...C=0.017670169402947963, penalty=l2, solver=lbfgs; total time=   4.4s\n",
      "[CV] END ...C=0.017670169402947963, penalty=l2, solver=lbfgs; total time=   3.7s\n",
      "[CV] END ...C=0.0012606912518374083, penalty=l2, solver=saga; total time=  11.8s\n",
      "[CV] END ...C=0.0012606912518374083, penalty=l2, solver=saga; total time= 4.8min\n",
      "[CV] END ...C=0.0012606912518374083, penalty=l2, solver=saga; total time= 4.9min\n",
      "[CV] END ....C=0.39079671568228835, penalty=l2, solver=lbfgs; total time=   9.9s\n",
      "[CV] END ....C=0.39079671568228835, penalty=l2, solver=lbfgs; total time=  10.8s\n",
      "[CV] END ....C=0.39079671568228835, penalty=l2, solver=lbfgs; total time=  10.0s\n",
      "[CV] END ...C=0.047314746448150063, penalty=l2, solver=lbfgs; total time=   5.2s\n",
      "[CV] END ...C=0.047314746448150063, penalty=l2, solver=lbfgs; total time=   5.1s\n",
      "[CV] END ...C=0.047314746448150063, penalty=l2, solver=lbfgs; total time=   5.2s\n",
      "[CV] END ..C=0.00022310108018679258, penalty=l2, solver=saga; total time=   5.0s\n",
      "[CV] END ..C=0.00022310108018679258, penalty=l2, solver=saga; total time= 5.1min\n",
      "[CV] END ..C=0.00022310108018679258, penalty=l2, solver=saga; total time= 5.1min\n",
      "\n",
      "Best parameters found (RandomizedSearchCV): {'C': np.float64(0.0012606912518374083), 'penalty': 'l2', 'solver': 'saga'}\n",
      "Best ROC AUC score (RandomizedSearchCV on sample data): 0.7471924443557819\n"
     ]
    }
   ],
   "source": [
    "# --- Inisialisasi dan Latih RandomizedSearchCV ---\n",
    "\n",
    "# n_iter adalah jumlah kombinasi parameter acak yang akan dicoba.\n",
    "# Mulai dengan nilai kecil (misalnya 5) dan tingkatkan jika ingin pencarian lebih menyeluruh.\n",
    "\n",
    "num_iterations_random_search = 5\n",
    "print(f\"\\n--- Memulai RandomizedSearchCV dengan {num_iterations_random_search} iterasi ---\")\n",
    "print(\"Ini akan menggunakan data sampel dan n_jobs=1 untuk menghemat memori.\")\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=model_lr_base,\n",
    "                                   param_distributions=param_distributions,\n",
    "                                   n_iter=num_iterations_random_search,\n",
    "                                   scoring='roc_auc', # Metrik yang ingin dioptimalkan\n",
    "                                   cv=folds, # Gunakan folds yang sudah didefinisikan\n",
    "                                   verbose=2, # Menampilkan progres\n",
    "                                   random_state=42,\n",
    "                                   n_jobs=1) # Sangat penting untuk tetap 1 guna menghindari MemoryError\n",
    "\n",
    "# Latih random search pada DATA SAMPEL\n",
    "# Ini adalah langkah yang harus berhasil diselesaikan sebelum 'best_model_lr' terdefinisi\n",
    "print(\"Menjalankan random_search.fit(). Ini mungkin memakan waktu...\")\n",
    "random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f\"\\nBest parameters found (RandomizedSearchCV): {random_search.best_params_}\")\n",
    "print(f\"Best ROC AUC score (RandomizedSearchCV on sample data): {random_search.best_score_}\")\n",
    "\n",
    "# best_model_lr akan terdefinisi setelah baris di atas selesai dieksekusi\n",
    "best_model_lr = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8dd3a76-d430-4f33-a4c3-965ca198f0a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Membuat Prediksi pada X_test menggunakan model terbaik dari tuning (dilatih pada sampel) ---\n"
     ]
    }
   ],
   "source": [
    "# --- Tahap Prediksi ---\n",
    "print(\"\\n--- Membuat Prediksi pada X_test menggunakan model terbaik dari tuning (dilatih pada sampel) ---\")\n",
    "final_predictions = best_model_lr.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "677bfbd8-caff-4e57-b033-be19aa02344e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Membuat File Submission ---\n",
      "Submission file 'submission_logistic_regression_tuned_sampled.csv' berhasil dibuat!\n",
      "   SK_ID_CURR    TARGET\n",
      "0      100001  0.432371\n",
      "1      100005  0.536147\n",
      "2      100013  0.361607\n",
      "3      100028  0.171090\n",
      "4      100038  0.690998\n",
      "\n",
      "--- Proses Selesai ---\n"
     ]
    }
   ],
   "source": [
    "# --- Buat File Submission ---\n",
    "print(\"\\n--- Membuat File Submission ---\")\n",
    "submission_lr_tuned = pd.DataFrame({'SK_ID_CURR': test_transformed['SK_ID_CURR'], 'TARGET': final_predictions})\n",
    "submission_lr_tuned.to_csv('submission_logistic_regression_tuned_sampled.csv', index=False)\n",
    "print(\"Submission file 'submission_logistic_regression_tuned_sampled.csv' berhasil dibuat!\")\n",
    "print(submission_lr_tuned.head())\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\n--- Proses Selesai ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922bde3f-f316-4a36-a23a-015f7d4455cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
