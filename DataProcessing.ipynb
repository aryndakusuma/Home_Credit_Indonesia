{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3e0754-eba6-437d-961d-0a62318535ac",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc70e10-d667-4f8f-b0b5-1a48da080efe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc # Garbage Collector to help manage memory\n",
    "\n",
    "from sklearn.impute import SimpleImputer # Untuk imputasi missing value\n",
    "from sklearn.ensemble import IsolationForest # Untuk deteksi outlier\n",
    "from scipy.stats import zscore # Untuk deteksi outlier berbasis Z-score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer # Ini baris yang perlu diubah/ditambahkan\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import loguniform # Untuk distribusi C\n",
    "import warnings\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49381f0a-1ff7-4626-8915-26723a155adb",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f6e1f-5a6b-48de-9309-69e5cc836f32",
   "metadata": {},
   "source": [
    "Optimasi Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3202e20-18fd-462e-af1d-34b3f30b3bc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Fungsi Utility ---\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else: # float types\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8eea6a-a5ea-4bfd-9213-24449661ad1e",
   "metadata": {},
   "source": [
    "Handle Missing Values & Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7789e543-07f6-4e33-a953-f5c0c13c2301",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Handle missing value & outliers\n",
    "\n",
    "def handle_specific_outliers(df, df_name):\n",
    "    \"\"\" Handles specific known outliers like DAYS_EMPLOYED anomaly. \"\"\"\n",
    "    if 'DAYS_EMPLOYED' in df.columns:\n",
    "        print(f\"  Handling specific outliers for DAYS_EMPLOYED in {df_name}...\")\n",
    "        df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
    "        df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n",
    "        print(f\"    Created DAYS_EMPLOYED_ANOM and replaced 365243 with NaN.\")\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df, df_name):\n",
    "    \"\"\" Fills missing values and potentially drops columns with too many NaNs. \"\"\"\n",
    "    print(f\"  Handling missing values in {df_name}...\")\n",
    "    threshold = 0.7 * len(df)\n",
    "    cols_to_drop = [col for col in df.columns if df[col].isnull().sum() > threshold]\n",
    "    if cols_to_drop:\n",
    "        print(f\"    Dropping {len(cols_to_drop)} columns with more than 70% missing values in {df_name}.\")\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    # Separate numerical and categorical columns for imputation\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    # Impute numerical columns with median, excluding IDs and TARGET\n",
    "    cols_to_impute_num = [col for col in numerical_cols if col not in ['SK_ID_CURR', 'SK_ID_PREV', 'SK_ID_BUREAU', 'TARGET']] \n",
    "    if cols_to_impute_num:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        df[cols_to_impute_num] = imputer_num.fit_transform(df[cols_to_impute_num])\n",
    "        print(f\"    Imputed {len(cols_to_impute_num)} numerical columns with median.\")\n",
    "\n",
    "    # Impute categorical columns with mode\n",
    "    if categorical_cols:\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
    "        print(f\"    Imputed {len(categorical_cols)} categorical columns with mode.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_all_numeric_outliers(df, df_name, id_cols=None):\n",
    "    \"\"\" Caps outliers for all numerical columns using IQR method (1% and 99% percentiles). \"\"\"\n",
    "    print(f\"  Handling numerical outliers in {df_name} using 1st and 99th percentile capping...\")\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    if id_cols is None:\n",
    "        id_cols = ['SK_ID_CURR'] \n",
    "    # Exclude ID columns and TARGET from outlier capping\n",
    "    cols_to_cap = [col for col in numerical_cols if col not in id_cols and col != 'TARGET']\n",
    "\n",
    "    for col in cols_to_cap:\n",
    "        # Check if column has enough non-NaN values to calculate percentiles\n",
    "        if df[col].count() > 0:\n",
    "            lower_bound = df[col].quantile(0.01)\n",
    "            upper_bound = df[col].quantile(0.99)\n",
    "            df[col] = np.clip(df[col], lower_bound, upper_bound)\n",
    "    print(f\"    Capped outliers for {len(cols_to_cap)} numerical columns.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469762b-ae5f-4f10-8114-b3a3a749ec00",
   "metadata": {},
   "source": [
    "Feature Engineering (Agregasi Data), Transformasi Fitur (Encoding), Final Data Preparation (Final Missing Values Check, StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c8063e-4f52-40bf-83be-e7a8df3bb4eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_and_aggregate_data():\n",
    "    print(\"\\n--- Loading main application data ---\")\n",
    "    app_train = pd.read_csv('application_train.csv')\n",
    "    app_test = pd.read_csv('application_test.csv')\n",
    "\n",
    "    print(f\"Initial app_train shape: {app_train.shape}\")\n",
    "    print(f\"Initial app_test shape: {app_test.shape}\")\n",
    "\n",
    "    # Tambahkan kolom 'TARGET' ke app_test dengan semua nilai NaN\n",
    "    app_test['TARGET'] = np.nan\n",
    "\n",
    "    # Gabungkan train dan test\n",
    "    df = pd.concat([app_train, app_test], ignore_index=True)\n",
    "    del app_train, app_test \n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Combined df shape after concat: {df.shape}\")\n",
    "\n",
    "    # Process combined application data\n",
    "    df = reduce_mem_usage(df)\n",
    "    df = handle_specific_outliers(df, 'Application Data')\n",
    "    df = handle_missing_values(df, 'Application Data')\n",
    "    df = handle_all_numeric_outliers(df, 'Application Data', id_cols=['SK_ID_CURR', 'TARGET'])\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Process Bureau and Bureau Balance ---\n",
    "    print(\"\\n--- Processing Bureau and Bureau Balance ---\")\n",
    "    bureau = pd.read_csv('bureau.csv')\n",
    "    bureau = reduce_mem_usage(bureau)\n",
    "    bureau = handle_missing_values(bureau, 'Bureau Data')\n",
    "    bureau = handle_all_numeric_outliers(bureau, 'Bureau Data', id_cols=['SK_ID_CURR', 'SK_ID_BUREAU'])\n",
    "\n",
    "    bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "    bureau_balance = reduce_mem_usage(bureau_balance)\n",
    "    bureau_balance = handle_missing_values(bureau_balance, 'Bureau Balance Data') # No specific outliers for BB\n",
    "\n",
    "    print(\"  Aggregating bureau_balance data...\")\n",
    "    # Debugging: Periksa kondisi bureau_balance sebelum agregasi\n",
    "    print(f\"  Shape of bureau_balance AFTER cleaning: {bureau_balance.shape}\")\n",
    "    print(f\"  Unique SK_ID_BUREAU in bureau_balance: {bureau_balance['SK_ID_BUREAU'].nunique()}\")\n",
    "    print(f\"  Is SK_ID_BUREAU present in bureau_balance? {'SK_ID_BUREAU' in bureau_balance.columns}\")\n",
    "    print(f\"  Data types of bureau_balance: \\n{bureau_balance.dtypes}\")\n",
    "    print(f\"  Categorical columns in bureau_balance: {bureau_balance.select_dtypes(include='category').columns.tolist()}\")\n",
    "    print(f\"  Numerical columns in bureau_balance: {bureau_balance.select_dtypes(include=np.number).columns.tolist()}\")\n",
    "\n",
    "    bb_numerical_cols = bureau_balance.select_dtypes(include=np.number).columns.drop(['SK_ID_BUREAU'], errors='ignore').tolist()\n",
    "    bb_categorical_cols = bureau_balance.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    bb_agg_dict = {}\n",
    "    # Agregasi kolom numerik (MONTHS_BALANCE adalah yang utama di sini)\n",
    "    for col in bb_numerical_cols:\n",
    "        bb_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'count', 'nunique']\n",
    "\n",
    "    # Agregasi kolom kategorikal (STATUS adalah yang utama di sini)\n",
    "    for col in bb_categorical_cols:\n",
    "        bb_agg_dict[col] = ['count', 'nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    # Periksa apakah dictionary agregasi kosong. Penting agar .agg() tidak error.\n",
    "    if not bb_agg_dict:\n",
    "        print(\"WARNING: Aggregation dictionary for bureau_balance is empty. This might indicate no relevant columns were found.\")\n",
    "        bureau_balance_agg = pd.DataFrame({'SK_ID_BUREAU': bureau_balance['SK_ID_BUREAU'].unique()})\n",
    "    else:\n",
    "        bureau_balance_agg = bureau_balance.groupby('SK_ID_BUREAU').agg(bb_agg_dict).reset_index()\n",
    "\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_BUREAU']\n",
    "    if not bureau_balance_agg.empty: # Only process if DataFrame is not empty\n",
    "        for col_tuple in bureau_balance_agg.columns.drop('SK_ID_BUREAU'):\n",
    "            if isinstance(col_tuple, tuple):\n",
    "                if col_tuple[1] == '<lambda>':\n",
    "                    new_cols.append(f'BB_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "                else:\n",
    "                    new_cols.append(f'BB_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "            else:\n",
    "                new_cols.append(f'BB_{str(col_tuple).upper()}')\n",
    "                \n",
    "        bureau_balance_agg.columns = new_cols\n",
    "    del bureau_balance\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated bureau_balance data. Shape: {bureau_balance_agg.shape}\")\n",
    "\n",
    "    # Merge aggregated bureau_balance to bureau\n",
    "    bureau = bureau.merge(bureau_balance_agg, on='SK_ID_BUREAU', how='left')\n",
    "    del bureau_balance_agg\n",
    "    gc.collect()\n",
    "    print(f\"  Merged bureau_balance to bureau. Bureau shape: {bureau.shape}\")\n",
    "\n",
    "    print(\"  Aggregating bureau data...\")\n",
    "    # Aggregate bureau data to SK_ID_CURR level\n",
    "    bureau_numerical_cols = bureau.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_BUREAU'], errors='ignore').tolist()\n",
    "    bureau_categorical_cols = bureau.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    bureau_agg_dict = {}\n",
    "    for col in bureau_numerical_cols:\n",
    "        bureau_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median', 'first', 'last']\n",
    "    for col in bureau_categorical_cols:\n",
    "        bureau_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg(bureau_agg_dict).reset_index()\n",
    "\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in bureau_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'BUREAU_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'BUREAU_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "             new_cols.append(f'BUREAU_{str(col_tuple).upper()}')\n",
    "\n",
    "    bureau_agg.columns = new_cols\n",
    "    del bureau\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated bureau data. Shape: {bureau_agg.shape}\")\n",
    "\n",
    "    # Merge aggregated bureau data to main df\n",
    "    df = df.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "    del bureau_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged bureau data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "    # --- Process Previous Application ---\n",
    "    print(\"\\n--- Processing Previous Application ---\")\n",
    "    previous_application = pd.read_csv('previous_application.csv')\n",
    "    previous_application = reduce_mem_usage(previous_application)\n",
    "    previous_application = handle_missing_values(previous_application, 'Previous Application Data')\n",
    "    previous_application = handle_all_numeric_outliers(previous_application, 'Previous Application Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating previous_application data...\")\n",
    "    # Aggregate previous_application\n",
    "    prev_num_cols = previous_application.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    prev_cat_cols = previous_application.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    prev_agg_dict = {}\n",
    "    for col in prev_num_cols:\n",
    "        prev_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in prev_cat_cols:\n",
    "        prev_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    previous_application_agg = previous_application.groupby('SK_ID_CURR').agg(prev_agg_dict).reset_index()\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in previous_application_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'PREV_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'PREV_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "            new_cols.append(f'PREV_{str(col_tuple).upper()}')\n",
    "\n",
    "    previous_application_agg.columns = new_cols\n",
    "    del previous_application\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated previous_application data. Shape: {previous_application_agg.shape}\")\n",
    "\n",
    "    df = df.merge(previous_application_agg, on='SK_ID_CURR', how='left')\n",
    "    del previous_application_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged previous_application data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "\n",
    "    # --- Process POS_CASH_balance ---\n",
    "    print(\"\\n--- Processing POS_CASH_balance ---\")\n",
    "    pos_cash = pd.read_csv('POS_CASH_balance.csv')\n",
    "    pos_cash = reduce_mem_usage(pos_cash)\n",
    "    pos_cash = handle_missing_values(pos_cash, 'POS_CASH_balance Data')\n",
    "    pos_cash = handle_all_numeric_outliers(pos_cash, 'POS_CASH_balance Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating POS_CASH_balance data...\")\n",
    "    # Aggregate POS_CASH_balance\n",
    "    pos_cash_num_cols = pos_cash.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    pos_cash_cat_cols = pos_cash.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    pos_cash_agg_dict = {}\n",
    "    for col in pos_cash_num_cols:\n",
    "        pos_cash_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in pos_cash_cat_cols:\n",
    "        pos_cash_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    pos_cash_agg = pos_cash.groupby('SK_ID_CURR').agg(pos_cash_agg_dict).reset_index()\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in pos_cash_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'POS_CASH_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'POS_CASH_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "            new_cols.append(f'POS_CASH_{str(col_tuple).upper()}')\n",
    "\n",
    "    pos_cash_agg.columns = new_cols\n",
    "    del pos_cash\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated POS_CASH_balance data. Shape: {pos_cash_agg.shape}\")\n",
    "\n",
    "    df = df.merge(pos_cash_agg, on='SK_ID_CURR', how='left')\n",
    "    del pos_cash_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged POS_CASH_balance data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "    # --- Process Credit Card Balance ---\n",
    "    print(\"\\n--- Processing Credit Card Balance ---\")\n",
    "    credit_card = pd.read_csv('credit_card_balance.csv')\n",
    "    credit_card = reduce_mem_usage(credit_card)\n",
    "    credit_card = handle_missing_values(credit_card, 'Credit Card Balance Data')\n",
    "    credit_card = handle_all_numeric_outliers(credit_card, 'Credit Card Balance Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating credit_card_balance data...\")\n",
    "\n",
    "    credit_card_numerical_cols = credit_card.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    credit_card_categorical_cols = credit_card.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    credit_card_agg_dict = {}\n",
    "    for col in credit_card_numerical_cols:\n",
    "        credit_card_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in credit_card_categorical_cols:\n",
    "        credit_card_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    credit_card_agg = credit_card.groupby('SK_ID_CURR').agg(credit_card_agg_dict).reset_index()\n",
    "\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in credit_card_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'CC_BAL_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'CC_BAL_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "             new_cols.append(f'CC_BAL_{str(col_tuple).upper()}')\n",
    "\n",
    "    credit_card_agg.columns = new_cols\n",
    "    del credit_card\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated credit_card_balance data. Shape: {credit_card_agg.shape}\")\n",
    "\n",
    "    df = df.merge(credit_card_agg, on='SK_ID_CURR', how='left')\n",
    "    del credit_card_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged credit_card_balance data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "\n",
    "    # --- Process Installments Payments ---\n",
    "    print(\"\\n--- Processing Installments Payments ---\")\n",
    "    installments_payments = pd.read_csv('installments_payments.csv')\n",
    "    installments_payments = reduce_mem_usage(installments_payments)\n",
    "    installments_payments = handle_missing_values(installments_payments, 'Installments Payments Data')\n",
    "    installments_payments = handle_all_numeric_outliers(installments_payments, 'Installments Payments Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating installments_payments data...\")\n",
    "    # Aggregate installments_payments\n",
    "    inst_num_cols = installments_payments.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    inst_cat_cols = installments_payments.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    inst_agg_dict = {}\n",
    "    for col in inst_num_cols:\n",
    "        inst_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in inst_cat_cols:\n",
    "        inst_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    installments_payments_agg = installments_payments.groupby('SK_ID_CURR').agg(inst_agg_dict).reset_index()\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in installments_payments_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'INSTAL_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'INSTAL_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "            new_cols.append(f'INSTAL_{str(col_tuple).upper()}')\n",
    "\n",
    "    installments_payments_agg.columns = new_cols\n",
    "    del installments_payments\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated installments_payments data. Shape: {installments_payments_agg.shape}\")\n",
    "\n",
    "    df = df.merge(installments_payments_agg, on='SK_ID_CURR', how='left')\n",
    "    del installments_payments_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged installments_payments data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "    # --- Final Data Transformation on the Combined DataFrame ---\n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"Starting Final Data Transformation (Encoding & Scaling)\")\n",
    "    print(\"=======================================================\")\n",
    "\n",
    "    # Handle categorical features (One-Hot Encoding for multi-valued categories)\n",
    "    # and Label Encoding for binary categories\n",
    "    print(\"  Encoding categorical features...\")\n",
    "    all_categorical_cols = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    # Pisahkan kolom biner dan multi-kelas\n",
    "    binary_cat_cols = [col for col in all_categorical_cols if df[col].nunique() == 2]\n",
    "    multi_cat_cols = [col for col in all_categorical_cols if df[col].nunique() > 2]\n",
    "\n",
    "    # Label Encode fitur kategorikal biner\n",
    "    for col in binary_cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        print(f\"    Label encoded: {col}\")\n",
    "\n",
    "    # One-Hot Encode fitur kategorikal multi-kelas\n",
    "    if multi_cat_cols:\n",
    "        print(f\"    One-Hot Encoding {len(multi_cat_cols)} multi-class categorical features...\")\n",
    "        df = pd.get_dummies(df, columns=multi_cat_cols, dummy_na=False)\n",
    "        print(f\"    DataFrame shape after One-Hot Encoding: {df.shape}\")\n",
    "    else:\n",
    "        print(\"    No multi-class categorical features to One-Hot Encode.\")\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"  Handling remaining missing values with 0 and infinite values...\")\n",
    "\n",
    "    # Identifikasi kolom numerik yang mungkin masih memiliki NaN setelah semua merge dan encoding\n",
    "    numerical_cols_for_final_fill = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['SK_ID_CURR', 'TARGET']]\n",
    "\n",
    "    # Isi NaN dengan 0 untuk kolom-kolom ini\n",
    "    df[numerical_cols_for_final_fill] = df[numerical_cols_for_final_fill].fillna(0)\n",
    "    print(f\"    Filled all remaining numerical NaNs with 0 for {len(numerical_cols_for_final_fill)} columns.\")\n",
    "\n",
    "    # Ganti nilai infinite dengan 0\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    print(\"    Replaced infinite values with 0.\")\n",
    "\n",
    "    # Optional: Verifikasi apakah masih ada NaN yang tersisa\n",
    "    if df.isnull().any().any():\n",
    "        print(\"WARNING: Some NaNs still exist after final fillna(0). Check these columns:\")\n",
    "        print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "\n",
    "    print(f\"  Final DataFrame shape after all preprocessing (before splitting): {df.shape}\")\n",
    "\n",
    "    print(\"  Re-optimizing memory usage before splitting and scaling...\")\n",
    "    df = reduce_mem_usage(df) # Jalankan lagi!\n",
    "    gc.collect()\n",
    "    \n",
    "    # Separate train and test data\n",
    "    train_df = df[df['TARGET'].notna()].copy()\n",
    "    test_df = df[df['TARGET'].isna()].drop(columns=['TARGET']).copy()\n",
    "\n",
    "    print(f\"\\nTrain DataFrame shape AFTER SPLIT: {train_df.shape}\")\n",
    "    print(f\"Test DataFrame shape AFTER SPLIT: {test_df.shape}\")\n",
    "\n",
    "    # Pastikan test_df memiliki SK_ID_CURR untuk submission\n",
    "    if 'SK_ID_CURR' not in test_df.columns:\n",
    "        print(\"WARNING: SK_ID_CURR not found in test_df after splitting. This might cause submission issues.\")\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Apply StandardScaler to numerical features\n",
    "    print(\"  Applying StandardScaler to numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    common_numerical_cols = [col for col in train_df.columns if col in test_df.columns and col not in ['SK_ID_CURR', 'TARGET'] and pd.api.types.is_numeric_dtype(train_df[col])]\n",
    "\n",
    "    train_df[common_numerical_cols] = scaler.fit_transform(train_df[common_numerical_cols].astype(np.float32))\n",
    "    test_df[common_numerical_cols] = scaler.transform(test_df[common_numerical_cols].astype(np.float32))\n",
    "    print(f\"  Scaled {len(common_numerical_cols)} numerical features.\")\n",
    "    gc.collect() \n",
    "    \n",
    "    print(f\"  Scaled {len(common_numerical_cols)} numerical features.\")\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d691bd2c-6b0a-459e-b965-6ca0ab18eb5a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading main application data ---\n",
      "Initial app_train shape: (307511, 122)\n",
      "Initial app_test shape: (48744, 121)\n",
      "Combined df shape after concat: (356255, 122)\n",
      "Memory usage of dataframe is 331.60 MB\n",
      "Memory usage after optimization is: 69.32 MB\n",
      "Decreased by 79.1%\n",
      "  Handling specific outliers for DAYS_EMPLOYED in Application Data...\n",
      "    Created DAYS_EMPLOYED_ANOM and replaced 365243 with NaN.\n",
      "  Handling missing values in Application Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\581809730.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\581809730.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Imputed 104 numerical columns with median.\n",
      "    Imputed 16 categorical columns with mode.\n",
      "  Handling numerical outliers in Application Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 104 numerical columns.\n",
      "\n",
      "--- Processing Bureau and Bureau Balance ---\n",
      "Memory usage of dataframe is 222.62 MB\n",
      "Memory usage after optimization is: 78.57 MB\n",
      "Decreased by 64.7%\n",
      "  Handling missing values in Bureau Data...\n",
      "    Dropping 1 columns with more than 70% missing values in Bureau Data.\n",
      "    Imputed 11 numerical columns with median.\n",
      "    Imputed 3 categorical columns with mode.\n",
      "  Handling numerical outliers in Bureau Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 11 numerical columns.\n",
      "Memory usage of dataframe is 624.85 MB\n",
      "Memory usage after optimization is: 156.21 MB\n",
      "Decreased by 75.0%\n",
      "  Handling missing values in Bureau Balance Data...\n",
      "    Imputed 1 numerical columns with median.\n",
      "    Imputed 1 categorical columns with mode.\n",
      "  Aggregating bureau_balance data...\n",
      "  Shape of bureau_balance AFTER cleaning: (27299925, 3)\n",
      "  Unique SK_ID_BUREAU in bureau_balance: 817395\n",
      "  Is SK_ID_BUREAU present in bureau_balance? True\n",
      "  Data types of bureau_balance: \n",
      "SK_ID_BUREAU        int32\n",
      "MONTHS_BALANCE    float64\n",
      "STATUS             object\n",
      "dtype: object\n",
      "  Categorical columns in bureau_balance: []\n",
      "  Numerical columns in bureau_balance: ['SK_ID_BUREAU', 'MONTHS_BALANCE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:72: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in bureau_balance_agg.columns.drop('SK_ID_BUREAU'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated bureau_balance data. Shape: (817395, 7)\n",
      "  Merged bureau_balance to bureau. Bureau shape: (1716428, 22)\n",
      "  Aggregating bureau data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:107: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in bureau_agg.columns.drop('SK_ID_CURR'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated bureau data. Shape: (305811, 154)\n",
      "Merged bureau data to main df. Main df shape: (356255, 276)\n",
      "\n",
      "--- Processing Previous Application ---\n",
      "Memory usage of dataframe is 471.48 MB\n",
      "Memory usage after optimization is: 130.62 MB\n",
      "Decreased by 72.3%\n",
      "  Handling missing values in Previous Application Data...\n",
      "    Dropping 2 columns with more than 70% missing values in Previous Application Data.\n",
      "    Imputed 17 numerical columns with median.\n",
      "    Imputed 16 categorical columns with mode.\n",
      "  Handling numerical outliers in Previous Application Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 17 numerical columns.\n",
      "  Aggregating previous_application data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:148: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in previous_application_agg.columns.drop('SK_ID_CURR'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated previous_application data. Shape: (338857, 120)\n",
      "Merged previous_application data to main df. Main df shape: (356255, 395)\n",
      "\n",
      "--- Processing POS_CASH_balance ---\n",
      "Memory usage of dataframe is 610.43 MB\n",
      "Memory usage after optimization is: 171.69 MB\n",
      "Decreased by 71.9%\n",
      "  Handling missing values in POS_CASH_balance Data...\n",
      "    Imputed 5 numerical columns with median.\n",
      "    Imputed 1 categorical columns with mode.\n",
      "  Handling numerical outliers in POS_CASH_balance Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 5 numerical columns.\n",
      "  Aggregating POS_CASH_balance data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:189: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in pos_cash_agg.columns.drop('SK_ID_CURR'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated POS_CASH_balance data. Shape: (337252, 36)\n",
      "Merged POS_CASH_balance data to main df. Main df shape: (356255, 430)\n",
      "\n",
      "--- Processing Credit Card Balance ---\n",
      "Memory usage of dataframe is 673.88 MB\n",
      "Memory usage after optimization is: 263.69 MB\n",
      "Decreased by 60.9%\n",
      "  Handling missing values in Credit Card Balance Data...\n",
      "    Imputed 20 numerical columns with median.\n",
      "    Imputed 1 categorical columns with mode.\n",
      "  Handling numerical outliers in Credit Card Balance Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 20 numerical columns.\n",
      "  Aggregating credit_card_balance data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:229: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in credit_card_agg.columns.drop('SK_ID_CURR'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated credit_card_balance data. Shape: (103558, 141)\n",
      "Merged credit_card_balance data to main df. Main df shape: (356255, 570)\n",
      "\n",
      "--- Processing Installments Payments ---\n",
      "Memory usage of dataframe is 830.41 MB\n",
      "Memory usage after optimization is: 311.40 MB\n",
      "Decreased by 62.5%\n",
      "  Handling missing values in Installments Payments Data...\n",
      "    Imputed 6 numerical columns with median.\n",
      "  Handling numerical outliers in Installments Payments Data using 1st and 99th percentile capping...\n",
      "    Capped outliers for 6 numerical columns.\n",
      "  Aggregating installments_payments data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11224\\3776969840.py:270: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  for col_tuple in installments_payments_agg.columns.drop('SK_ID_CURR'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Aggregated installments_payments data. Shape: (339587, 43)\n",
      "Merged installments_payments data to main df. Main df shape: (356255, 612)\n",
      "\n",
      "=======================================================\n",
      "Starting Final Data Transformation (Encoding & Scaling)\n",
      "=======================================================\n",
      "  Encoding categorical features...\n",
      "    No multi-class categorical features to One-Hot Encode.\n",
      "  Handling remaining missing values with 0 and infinite values...\n",
      "    Filled all remaining numerical NaNs with 0 for 594 columns.\n",
      "    Replaced infinite values with 0.\n",
      "WARNING: Some NaNs still exist after final fillna(0). Check these columns:\n",
      "TARGET    48744\n",
      "dtype: int64\n",
      "  Final DataFrame shape after all preprocessing (before splitting): (356255, 612)\n",
      "  Re-optimizing memory usage before splitting and scaling...\n",
      "Memory usage of dataframe is 1573.39 MB\n",
      "Memory usage after optimization is: 519.83 MB\n",
      "Decreased by 67.0%\n",
      "\n",
      "Train DataFrame shape AFTER SPLIT: (307511, 612)\n",
      "Test DataFrame shape AFTER SPLIT: (48744, 611)\n",
      "  Applying StandardScaler to numerical features...\n",
      "  Scaled 594 numerical features.\n",
      "  Scaled 594 numerical features.\n",
      "\n",
      "--- Transformed Train Data Head (Sample) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1458: RuntimeWarning: overflow encountered in cast\n",
      "  has_large_values = (abs_vals > 1e6).any()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
      "0      100002     1.0         Cash loans           M            N   \n",
      "1      100003     0.0         Cash loans           F            N   \n",
      "2      100004     0.0    Revolving loans           M            Y   \n",
      "3      100006     0.0         Cash loans           F            N   \n",
      "4      100007     0.0         Cash loans           M            N   \n",
      "\n",
      "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
      "0               Y     -0.586054          0.434759   -0.484740    -0.164808   \n",
      "1               N     -0.586054          1.243180    1.780900     0.638160   \n",
      "2               Y     -0.586054         -1.182084   -1.178548    -1.475380   \n",
      "3               Y     -0.586054         -0.373663   -0.724650     0.199222   \n",
      "4               Y     -0.586054         -0.535347   -0.212930    -0.371792   \n",
      "\n",
      "   ...  INSTAL_AMT_INSTALMENT_STD INSTAL_AMT_INSTALMENT_COUNT  \\\n",
      "0  ...                  -0.122802                   -0.460054   \n",
      "1  ...                   2.601186                   -0.312403   \n",
      "2  ...                  -0.684826                   -0.853791   \n",
      "3  ...                   1.737069                   -0.533880   \n",
      "4  ...                  -0.298690                    0.696548   \n",
      "\n",
      "  INSTAL_AMT_INSTALMENT_MEDIAN INSTAL_AMT_PAYMENT_MIN INSTAL_AMT_PAYMENT_MAX  \\\n",
      "0                    -0.230117               0.504224              -0.185409   \n",
      "1                     4.456307               0.209558               1.696012   \n",
      "2                    -0.561817               0.060936              -0.826068   \n",
      "3                     1.454199              -0.266229               1.696012   \n",
      "4                     0.347841              -0.548413              -0.643681   \n",
      "\n",
      "  INSTAL_AMT_PAYMENT_MEAN  INSTAL_AMT_PAYMENT_SUM  INSTAL_AMT_PAYMENT_STD  \\\n",
      "0               -0.253276               -0.471933               -0.233453   \n",
      "1                2.546907                1.003667                2.227077   \n",
      "2               -0.583196               -0.759888               -0.696617   \n",
      "3                1.171473               -0.074561                1.796436   \n",
      "4               -0.204867                0.379581               -0.365581   \n",
      "\n",
      "   INSTAL_AMT_PAYMENT_COUNT  INSTAL_AMT_PAYMENT_MEDIAN  \n",
      "0                 -0.460054                  -0.200059  \n",
      "1                 -0.312403                   4.277330  \n",
      "2                 -0.853791                  -0.516964  \n",
      "3                 -0.533880                   1.409129  \n",
      "4                  0.696548                   0.352119  \n",
      "\n",
      "[5 rows x 612 columns]\n",
      "\n",
      "--- Transformed Test Data Head (Sample) ---\n",
      "        SK_ID_CURR NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
      "307511      100001         Cash loans           F            N   \n",
      "307512      100005         Cash loans           M            N   \n",
      "307513      100013         Cash loans           M            Y   \n",
      "307514      100028         Cash loans           F            N   \n",
      "307515      100038         Cash loans           M            Y   \n",
      "\n",
      "       FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
      "307511               Y     -0.586054         -0.373663   -0.070386   \n",
      "307512               Y     -0.586054         -0.804821   -0.954341   \n",
      "307513               Y     -0.586054          0.434759    0.170927   \n",
      "307514               Y      2.242382          1.782127    2.499998   \n",
      "307515               N      0.828164          0.165285    0.074457   \n",
      "\n",
      "        AMT_ANNUITY  AMT_GOODS_PRICE  ... INSTAL_AMT_INSTALMENT_STD  \\\n",
      "307511    -0.467070        -0.239637  ...                 -0.520131   \n",
      "307512    -0.700010        -0.986295  ...                 -0.583595   \n",
      "307513     3.126244         0.258135  ...                  0.623563   \n",
      "307514     1.610658         2.871439  ...                 -0.530845   \n",
      "307515     0.373023         0.245691  ...                 -0.924989   \n",
      "\n",
      "       INSTAL_AMT_INSTALMENT_COUNT INSTAL_AMT_INSTALMENT_MEDIAN  \\\n",
      "307511                   -0.755356                    -0.679039   \n",
      "307512                   -0.706139                    -0.608154   \n",
      "307513                    2.886708                    -0.922281   \n",
      "307514                    1.853149                    -0.515801   \n",
      "307515                   -0.632314                    -0.072651   \n",
      "\n",
      "       INSTAL_AMT_PAYMENT_MIN INSTAL_AMT_PAYMENT_MAX  INSTAL_AMT_PAYMENT_MEAN  \\\n",
      "307511              -0.099127              -0.723250                -0.672717   \n",
      "307512              -0.000989              -0.719357                -0.646469   \n",
      "307513              -0.548141               1.696012                -0.473293   \n",
      "307514              -0.548413              -0.397937                -0.785686   \n",
      "307515               0.714304              -0.818133                -0.287200   \n",
      "\n",
      "        INSTAL_AMT_PAYMENT_SUM  INSTAL_AMT_PAYMENT_STD  \\\n",
      "307511               -0.730985               -0.560892   \n",
      "307512               -0.709257               -0.613193   \n",
      "307513                1.140662                0.478116   \n",
      "307514               -0.076010               -0.582905   \n",
      "307515               -0.597404               -0.894536   \n",
      "\n",
      "        INSTAL_AMT_PAYMENT_COUNT  INSTAL_AMT_PAYMENT_MEDIAN  \n",
      "307511                 -0.755356                  -0.628958  \n",
      "307512                 -0.706139                  -0.561234  \n",
      "307513                  2.886708                  -0.896865  \n",
      "307514                  1.853149                  -0.574490  \n",
      "307515                 -0.632314                  -0.049617  \n",
      "\n",
      "[5 rows x 611 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Cara Memanggil Fungsi (Uncomment untuk menjalankan) ---\n",
    "train_transformed, test_transformed = process_and_aggregate_data()\n",
    "\n",
    "# # Tampilkan hasil (opsional, untuk verifikasi)\n",
    "print(\"\\n--- Transformed Train Data Head (Sample) ---\")\n",
    "print(train_transformed.head())\n",
    "print(\"\\n--- Transformed Test Data Head (Sample) ---\")\n",
    "print(test_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b241f-2483-46a4-9fed-2823efad64a2",
   "metadata": {},
   "source": [
    "Result Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81c5bfc-2f3d-4fd0-9e44-7d710876be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan\n",
    "train_transformed.to_csv('train_transformed.csv', index=False)\n",
    "test_transformed.to_csv('test_transformed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922bde3f-f316-4a36-a23a-015f7d4455cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
