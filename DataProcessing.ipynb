{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3e0754-eba6-437d-961d-0a62318535ac",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc70e10-d667-4f8f-b0b5-1a48da080efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "from sklearn.impute import SimpleImputer # Untuk imputasi missing value\n",
    "from sklearn.ensemble import IsolationForest # Untuk deteksi outlier\n",
    "from scipy.stats import zscore # Untuk deteksi outlier berbasis Z-score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49381f0a-1ff7-4626-8915-26723a155adb",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f6e1f-5a6b-48de-9309-69e5cc836f32",
   "metadata": {},
   "source": [
    "Optimasi Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3202e20-18fd-462e-af1d-34b3f30b3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fungsi Utility ---\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else: # float types\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8eea6a-a5ea-4bfd-9213-24449661ad1e",
   "metadata": {},
   "source": [
    "Handle Missing Values & Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7789e543-07f6-4e33-a953-f5c0c13c2301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing value & outliers\n",
    "\n",
    "def handle_specific_outliers(df, df_name):\n",
    "    \"\"\" Handles specific known outliers like DAYS_EMPLOYED anomaly. \"\"\"\n",
    "    if 'DAYS_EMPLOYED' in df.columns:\n",
    "        print(f\"  Handling specific outliers for DAYS_EMPLOYED in {df_name}...\")\n",
    "        df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
    "        df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n",
    "        print(f\"    Created DAYS_EMPLOYED_ANOM and replaced 365243 with NaN.\")\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df, df_name):\n",
    "    \"\"\" Fills missing values and potentially drops columns with too many NaNs. \"\"\"\n",
    "    print(f\"  Handling missing values in {df_name}...\")\n",
    "    threshold = 0.7 * len(df)\n",
    "    cols_to_drop = [col for col in df.columns if df[col].isnull().sum() > threshold]\n",
    "    if cols_to_drop:\n",
    "        print(f\"    Dropping {len(cols_to_drop)} columns with more than 70% missing values in {df_name}.\")\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    # Separate numerical and categorical columns for imputation\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    # Impute numerical columns with median, excluding IDs and TARGET\n",
    "    cols_to_impute_num = [col for col in numerical_cols if col not in ['SK_ID_CURR', 'SK_ID_PREV', 'SK_ID_BUREAU', 'TARGET']] \n",
    "    if cols_to_impute_num:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        df[cols_to_impute_num] = imputer_num.fit_transform(df[cols_to_impute_num])\n",
    "        print(f\"    Imputed {len(cols_to_impute_num)} numerical columns with median.\")\n",
    "\n",
    "    # Impute categorical columns with mode\n",
    "    if categorical_cols:\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
    "        print(f\"    Imputed {len(categorical_cols)} categorical columns with mode.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def handle_all_numeric_outliers(df, df_name, id_cols=None):\n",
    "    \"\"\" Caps outliers for all numerical columns using IQR method (1% and 99% percentiles). \"\"\"\n",
    "    print(f\"  Handling numerical outliers in {df_name} using 1st and 99th percentile capping...\")\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    if id_cols is None:\n",
    "        id_cols = ['SK_ID_CURR'] \n",
    "    # Exclude ID columns and TARGET from outlier capping\n",
    "    cols_to_cap = [col for col in numerical_cols if col not in id_cols and col != 'TARGET']\n",
    "\n",
    "    for col in cols_to_cap:\n",
    "        # Check if column has enough non-NaN values to calculate percentiles\n",
    "        if df[col].count() > 0:\n",
    "            lower_bound = df[col].quantile(0.01)\n",
    "            upper_bound = df[col].quantile(0.99)\n",
    "            df[col] = np.clip(df[col], lower_bound, upper_bound)\n",
    "    print(f\"    Capped outliers for {len(cols_to_cap)} numerical columns.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469762b-ae5f-4f10-8114-b3a3a749ec00",
   "metadata": {},
   "source": [
    "Feature Engineering (Agregasi Data), Transformasi Fitur (Encoding), Final Data Preparation (Final Missing Values Check, StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c8063e-4f52-40bf-83be-e7a8df3bb4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_aggregate_data():\n",
    "    print(\"\\n--- Loading main application data ---\")\n",
    "    app_train = pd.read_csv('application_train.csv')\n",
    "    app_test = pd.read_csv('application_test.csv')\n",
    "\n",
    "    print(f\"Initial app_train shape: {app_train.shape}\")\n",
    "    print(f\"Initial app_test shape: {app_test.shape}\")\n",
    "\n",
    "    # Tambahkan kolom 'TARGET' ke app_test dengan semua nilai NaN\n",
    "    app_test['TARGET'] = np.nan\n",
    "\n",
    "    # Gabungkan train dan test\n",
    "    df = pd.concat([app_train, app_test], ignore_index=True)\n",
    "    del app_train, app_test \n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Combined df shape after concat: {df.shape}\")\n",
    "\n",
    "    # Process combined application data\n",
    "    df = reduce_mem_usage(df)\n",
    "    df = handle_specific_outliers(df, 'Application Data')\n",
    "    df = handle_missing_values(df, 'Application Data')\n",
    "    df = handle_all_numeric_outliers(df, 'Application Data', id_cols=['SK_ID_CURR', 'TARGET'])\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Process Bureau and Bureau Balance ---\n",
    "    print(\"\\n--- Processing Bureau and Bureau Balance ---\")\n",
    "    bureau = pd.read_csv('bureau.csv')\n",
    "    bureau = reduce_mem_usage(bureau)\n",
    "    bureau = handle_missing_values(bureau, 'Bureau Data')\n",
    "    bureau = handle_all_numeric_outliers(bureau, 'Bureau Data', id_cols=['SK_ID_CURR', 'SK_ID_BUREAU'])\n",
    "\n",
    "    bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "    bureau_balance = reduce_mem_usage(bureau_balance)\n",
    "    bureau_balance = handle_missing_values(bureau_balance, 'Bureau Balance Data') # No specific outliers for BB\n",
    "\n",
    "    print(\"  Aggregating bureau_balance data...\")\n",
    "    # Debugging: Periksa kondisi bureau_balance sebelum agregasi\n",
    "    print(f\"  Shape of bureau_balance AFTER cleaning: {bureau_balance.shape}\")\n",
    "    print(f\"  Unique SK_ID_BUREAU in bureau_balance: {bureau_balance['SK_ID_BUREAU'].nunique()}\")\n",
    "    print(f\"  Is SK_ID_BUREAU present in bureau_balance? {'SK_ID_BUREAU' in bureau_balance.columns}\")\n",
    "    print(f\"  Data types of bureau_balance: \\n{bureau_balance.dtypes}\")\n",
    "    print(f\"  Categorical columns in bureau_balance: {bureau_balance.select_dtypes(include='category').columns.tolist()}\")\n",
    "    print(f\"  Numerical columns in bureau_balance: {bureau_balance.select_dtypes(include=np.number).columns.tolist()}\")\n",
    "\n",
    "    bb_numerical_cols = bureau_balance.select_dtypes(include=np.number).columns.drop(['SK_ID_BUREAU'], errors='ignore').tolist()\n",
    "    bb_categorical_cols = bureau_balance.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    bb_agg_dict = {}\n",
    "    # Agregasi kolom numerik (MONTHS_BALANCE adalah yang utama di sini)\n",
    "    for col in bb_numerical_cols:\n",
    "        bb_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'count', 'nunique']\n",
    "\n",
    "    # Agregasi kolom kategorikal (STATUS adalah yang utama di sini)\n",
    "    for col in bb_categorical_cols:\n",
    "        bb_agg_dict[col] = ['count', 'nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    # Periksa apakah dictionary agregasi kosong. Penting agar .agg() tidak error.\n",
    "    if not bb_agg_dict:\n",
    "        print(\"WARNING: Aggregation dictionary for bureau_balance is empty. This might indicate no relevant columns were found.\")\n",
    "        bureau_balance_agg = pd.DataFrame({'SK_ID_BUREAU': bureau_balance['SK_ID_BUREAU'].unique()})\n",
    "    else:\n",
    "        bureau_balance_agg = bureau_balance.groupby('SK_ID_BUREAU').agg(bb_agg_dict).reset_index()\n",
    "\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_BUREAU']\n",
    "    if not bureau_balance_agg.empty: # Only process if DataFrame is not empty\n",
    "        for col_tuple in bureau_balance_agg.columns.drop('SK_ID_BUREAU'):\n",
    "            if isinstance(col_tuple, tuple):\n",
    "                if col_tuple[1] == '<lambda>':\n",
    "                    new_cols.append(f'BB_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "                else:\n",
    "                    new_cols.append(f'BB_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "            else:\n",
    "                new_cols.append(f'BB_{str(col_tuple).upper()}')\n",
    "                \n",
    "        bureau_balance_agg.columns = new_cols\n",
    "    del bureau_balance\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated bureau_balance data. Shape: {bureau_balance_agg.shape}\")\n",
    "\n",
    "    # Merge aggregated bureau_balance to bureau\n",
    "    bureau = bureau.merge(bureau_balance_agg, on='SK_ID_BUREAU', how='left')\n",
    "    del bureau_balance_agg\n",
    "    gc.collect()\n",
    "    print(f\"  Merged bureau_balance to bureau. Bureau shape: {bureau.shape}\")\n",
    "\n",
    "    print(\"  Aggregating bureau data...\")\n",
    "    # Aggregate bureau data to SK_ID_CURR level\n",
    "    bureau_numerical_cols = bureau.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_BUREAU'], errors='ignore').tolist()\n",
    "    bureau_categorical_cols = bureau.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    bureau_agg_dict = {}\n",
    "    for col in bureau_numerical_cols:\n",
    "        bureau_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median', 'first', 'last']\n",
    "    for col in bureau_categorical_cols:\n",
    "        bureau_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg(bureau_agg_dict).reset_index()\n",
    "\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in bureau_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'BUREAU_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'BUREAU_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "             new_cols.append(f'BUREAU_{str(col_tuple).upper()}')\n",
    "\n",
    "    bureau_agg.columns = new_cols\n",
    "    del bureau\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated bureau data. Shape: {bureau_agg.shape}\")\n",
    "\n",
    "    # Merge aggregated bureau data to main df\n",
    "    df = df.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "    del bureau_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged bureau data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "    # --- Process Previous Application ---\n",
    "    print(\"\\n--- Processing Previous Application ---\")\n",
    "    previous_application = pd.read_csv('previous_application.csv')\n",
    "    previous_application = reduce_mem_usage(previous_application)\n",
    "    previous_application = handle_missing_values(previous_application, 'Previous Application Data')\n",
    "    previous_application = handle_all_numeric_outliers(previous_application, 'Previous Application Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating previous_application data...\")\n",
    "    # Aggregate previous_application\n",
    "    prev_num_cols = previous_application.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    prev_cat_cols = previous_application.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    prev_agg_dict = {}\n",
    "    for col in prev_num_cols:\n",
    "        prev_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in prev_cat_cols:\n",
    "        prev_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    previous_application_agg = previous_application.groupby('SK_ID_CURR').agg(prev_agg_dict).reset_index()\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in previous_application_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'PREV_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'PREV_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "            new_cols.append(f'PREV_{str(col_tuple).upper()}')\n",
    "\n",
    "    previous_application_agg.columns = new_cols\n",
    "    del previous_application\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated previous_application data. Shape: {previous_application_agg.shape}\")\n",
    "\n",
    "    df = df.merge(previous_application_agg, on='SK_ID_CURR', how='left')\n",
    "    del previous_application_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged previous_application data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "\n",
    "    # --- Process POS_CASH_balance ---\n",
    "    print(\"\\n--- Processing POS_CASH_balance ---\")\n",
    "    pos_cash = pd.read_csv('POS_CASH_balance.csv')\n",
    "    pos_cash = reduce_mem_usage(pos_cash)\n",
    "    pos_cash = handle_missing_values(pos_cash, 'POS_CASH_balance Data')\n",
    "    pos_cash = handle_all_numeric_outliers(pos_cash, 'POS_CASH_balance Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating POS_CASH_balance data...\")\n",
    "    # Aggregate POS_CASH_balance\n",
    "    pos_cash_num_cols = pos_cash.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    pos_cash_cat_cols = pos_cash.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    pos_cash_agg_dict = {}\n",
    "    for col in pos_cash_num_cols:\n",
    "        pos_cash_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in pos_cash_cat_cols:\n",
    "        pos_cash_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    pos_cash_agg = pos_cash.groupby('SK_ID_CURR').agg(pos_cash_agg_dict).reset_index()\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in pos_cash_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'POS_CASH_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'POS_CASH_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "            new_cols.append(f'POS_CASH_{str(col_tuple).upper()}')\n",
    "\n",
    "    pos_cash_agg.columns = new_cols\n",
    "    del pos_cash\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated POS_CASH_balance data. Shape: {pos_cash_agg.shape}\")\n",
    "\n",
    "    df = df.merge(pos_cash_agg, on='SK_ID_CURR', how='left')\n",
    "    del pos_cash_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged POS_CASH_balance data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "    # --- Process Credit Card Balance ---\n",
    "    print(\"\\n--- Processing Credit Card Balance ---\")\n",
    "    credit_card = pd.read_csv('credit_card_balance.csv')\n",
    "    credit_card = reduce_mem_usage(credit_card)\n",
    "    credit_card = handle_missing_values(credit_card, 'Credit Card Balance Data')\n",
    "    credit_card = handle_all_numeric_outliers(credit_card, 'Credit Card Balance Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating credit_card_balance data...\")\n",
    "\n",
    "    credit_card_numerical_cols = credit_card.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    credit_card_categorical_cols = credit_card.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    credit_card_agg_dict = {}\n",
    "    for col in credit_card_numerical_cols:\n",
    "        credit_card_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in credit_card_categorical_cols:\n",
    "        credit_card_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    credit_card_agg = credit_card.groupby('SK_ID_CURR').agg(credit_card_agg_dict).reset_index()\n",
    "\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in credit_card_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'CC_BAL_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'CC_BAL_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "             new_cols.append(f'CC_BAL_{str(col_tuple).upper()}')\n",
    "\n",
    "    credit_card_agg.columns = new_cols\n",
    "    del credit_card\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated credit_card_balance data. Shape: {credit_card_agg.shape}\")\n",
    "\n",
    "    df = df.merge(credit_card_agg, on='SK_ID_CURR', how='left')\n",
    "    del credit_card_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged credit_card_balance data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "\n",
    "    # --- Process Installments Payments ---\n",
    "    print(\"\\n--- Processing Installments Payments ---\")\n",
    "    installments_payments = pd.read_csv('installments_payments.csv')\n",
    "    installments_payments = reduce_mem_usage(installments_payments)\n",
    "    installments_payments = handle_missing_values(installments_payments, 'Installments Payments Data')\n",
    "    installments_payments = handle_all_numeric_outliers(installments_payments, 'Installments Payments Data', id_cols=['SK_ID_CURR', 'SK_ID_PREV'])\n",
    "\n",
    "    print(\"  Aggregating installments_payments data...\")\n",
    "    # Aggregate installments_payments\n",
    "    inst_num_cols = installments_payments.select_dtypes(include=np.number).columns.drop(['SK_ID_CURR', 'SK_ID_PREV'], errors='ignore').tolist()\n",
    "    inst_cat_cols = installments_payments.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    inst_agg_dict = {}\n",
    "    for col in inst_num_cols:\n",
    "        inst_agg_dict[col] = ['min', 'max', 'mean', 'sum', 'std', 'count', 'median']\n",
    "    for col in inst_cat_cols:\n",
    "        inst_agg_dict[col] = ['nunique', (f'{col}_mode', lambda x: x.mode()[0] if not x.mode().empty else np.nan)]\n",
    "\n",
    "    installments_payments_agg = installments_payments.groupby('SK_ID_CURR').agg(inst_agg_dict).reset_index()\n",
    "    # Flatten multi-level columns\n",
    "    new_cols = ['SK_ID_CURR']\n",
    "    for col_tuple in installments_payments_agg.columns.drop('SK_ID_CURR'):\n",
    "        if isinstance(col_tuple, tuple):\n",
    "            if col_tuple[1] == '<lambda>':\n",
    "                new_cols.append(f'INSTAL_{col_tuple[0].upper()}_{col_tuple[1].upper().replace(\"<LAMBDA>\", \"MODE\")}')\n",
    "            else:\n",
    "                new_cols.append(f'INSTAL_{col_tuple[0].upper()}_{col_tuple[1].upper()}')\n",
    "        else:\n",
    "            new_cols.append(f'INSTAL_{str(col_tuple).upper()}')\n",
    "\n",
    "    installments_payments_agg.columns = new_cols\n",
    "    del installments_payments\n",
    "    gc.collect()\n",
    "    print(f\"  Aggregated installments_payments data. Shape: {installments_payments_agg.shape}\")\n",
    "\n",
    "    df = df.merge(installments_payments_agg, on='SK_ID_CURR', how='left')\n",
    "    del installments_payments_agg\n",
    "    gc.collect()\n",
    "    print(f\"Merged installments_payments data to main df. Main df shape: {df.shape}\")\n",
    "\n",
    "    # --- Final Data Transformation on the Combined DataFrame ---\n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"Starting Final Data Transformation (Encoding & Scaling)\")\n",
    "    print(\"=======================================================\")\n",
    "\n",
    "    # Handle categorical features (One-Hot Encoding for multi-valued categories)\n",
    "    # and Label Encoding for binary categories\n",
    "    print(\"  Encoding categorical features...\")\n",
    "    all_categorical_cols = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    # Pisahkan kolom biner dan multi-kelas\n",
    "    binary_cat_cols = [col for col in all_categorical_cols if df[col].nunique() == 2]\n",
    "    multi_cat_cols = [col for col in all_categorical_cols if df[col].nunique() > 2]\n",
    "\n",
    "    # Label Encode fitur kategorikal biner\n",
    "    for col in binary_cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        print(f\"    Label encoded: {col}\")\n",
    "\n",
    "    # One-Hot Encode fitur kategorikal multi-kelas\n",
    "    if multi_cat_cols:\n",
    "        print(f\"    One-Hot Encoding {len(multi_cat_cols)} multi-class categorical features...\")\n",
    "        df = pd.get_dummies(df, columns=multi_cat_cols, dummy_na=False)\n",
    "        print(f\"    DataFrame shape after One-Hot Encoding: {df.shape}\")\n",
    "    else:\n",
    "        print(\"    No multi-class categorical features to One-Hot Encode.\")\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"  Handling remaining missing values with 0 and infinite values...\")\n",
    "\n",
    "    # Identifikasi kolom numerik yang mungkin masih memiliki NaN setelah semua merge dan encoding\n",
    "    numerical_cols_for_final_fill = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['SK_ID_CURR', 'TARGET']]\n",
    "\n",
    "    # Isi NaN dengan 0 untuk kolom-kolom ini\n",
    "    df[numerical_cols_for_final_fill] = df[numerical_cols_for_final_fill].fillna(0)\n",
    "    print(f\"    Filled all remaining numerical NaNs with 0 for {len(numerical_cols_for_final_fill)} columns.\")\n",
    "\n",
    "    # Ganti nilai infinite dengan 0\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    print(\"    Replaced infinite values with 0.\")\n",
    "\n",
    "    # Optional: Verifikasi apakah masih ada NaN yang tersisa\n",
    "    if df.isnull().any().any():\n",
    "        print(\"WARNING: Some NaNs still exist after final fillna(0). Check these columns:\")\n",
    "        print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "\n",
    "    print(f\"  Final DataFrame shape after all preprocessing (before splitting): {df.shape}\")\n",
    "\n",
    "    print(\"  Re-optimizing memory usage before splitting and scaling...\")\n",
    "    df = reduce_mem_usage(df) # Jalankan lagi!\n",
    "    gc.collect()\n",
    "    \n",
    "    # Separate train and test data\n",
    "    train_df = df[df['TARGET'].notna()].copy()\n",
    "    test_df = df[df['TARGET'].isna()].drop(columns=['TARGET']).copy()\n",
    "\n",
    "    print(f\"\\nTrain DataFrame shape AFTER SPLIT: {train_df.shape}\")\n",
    "    print(f\"Test DataFrame shape AFTER SPLIT: {test_df.shape}\")\n",
    "\n",
    "    # Pastikan test_df memiliki SK_ID_CURR untuk submission\n",
    "    if 'SK_ID_CURR' not in test_df.columns:\n",
    "        print(\"WARNING: SK_ID_CURR not found in test_df after splitting. This might cause submission issues.\")\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Apply StandardScaler to numerical features\n",
    "    print(\"  Applying StandardScaler to numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    common_numerical_cols = [col for col in train_df.columns if col in test_df.columns and col not in ['SK_ID_CURR', 'TARGET'] and pd.api.types.is_numeric_dtype(train_df[col])]\n",
    "\n",
    "    train_df[common_numerical_cols] = scaler.fit_transform(train_df[common_numerical_cols].astype(np.float32))\n",
    "    test_df[common_numerical_cols] = scaler.transform(test_df[common_numerical_cols].astype(np.float32))\n",
    "    print(f\"  Scaled {len(common_numerical_cols)} numerical features.\")\n",
    "    gc.collect() \n",
    "    \n",
    "    print(f\"  Scaled {len(common_numerical_cols)} numerical features.\")\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691bd2c-6b0a-459e-b965-6ca0ab18eb5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading main application data ---\n",
      "Initial app_train shape: (307511, 122)\n",
      "Initial app_test shape: (48744, 121)\n",
      "Combined df shape after concat: (356255, 122)\n",
      "Memory usage of dataframe is 331.60 MB\n",
      "Memory usage after optimization is: 69.32 MB\n",
      "Decreased by 79.1%\n",
      "  Handling specific outliers for DAYS_EMPLOYED in Application Data...\n",
      "    Created DAYS_EMPLOYED_ANOM and replaced 365243 with NaN.\n",
      "  Handling missing values in Application Data...\n"
     ]
    }
   ],
   "source": [
    "# --- Cara Memanggil Fungsi (Uncomment untuk menjalankan) ---\n",
    "train_transformed, test_transformed = process_and_aggregate_data()\n",
    "\n",
    "# # Tampilkan hasil (opsional, untuk verifikasi)\n",
    "print(\"\\n--- Transformed Train Data Head (Sample) ---\")\n",
    "print(train_transformed.head())\n",
    "print(\"\\n--- Transformed Test Data Head (Sample) ---\")\n",
    "print(test_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b241f-2483-46a4-9fed-2823efad64a2",
   "metadata": {},
   "source": [
    "Result Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c5bfc-2f3d-4fd0-9e44-7d710876be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan\n",
    "train_transformed.to_csv('train_transformed.csv', index=False)\n",
    "test_transformed.to_csv('test_transformed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922bde3f-f316-4a36-a23a-015f7d4455cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
